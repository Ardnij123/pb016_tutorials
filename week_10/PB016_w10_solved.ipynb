{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8Cj_X1VnmC0"
   },
   "source": [
    "# PB016: Artificial Intelligence I, labs 10 - Natural language processing\n",
    "\n",
    "This week's topic is natural language processing (NLP). We'll focus namely on:\n",
    "1. __Text acquisition and pre-processing__\n",
    "2. __Tokenization, tagging and stemming (dummy pipeline)__\n",
    "3. __The NLTK NLP library, shallow syntactic analysis (smarter pipeline)__\n",
    "4. __Sentiment analysis__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjLOiihWnmC1"
   },
   "source": [
    "---\n",
    "\n",
    "## 1. Text acquisition and preprocessing\n",
    "\n",
    "__Basic facts__\n",
    "- Usually a necessary step before applying the natural language processing methods themselves.\n",
    "- It consists mainly of preparing texts for machine processing (e.g., removal of OCR noise, markup, splitting in segments, normalisation, etc.).\n",
    "\n",
    "__Examples of typical tasks__\n",
    "- Conversion and cleaning of text - obtaining material (e.g., by downloading), encoding transformations, conversion from the original format to plain text, possible removal of noise in the form of OCR errors, formatting and other marks, annotation passages, etc.\n",
    "- Removal of \"irrelevant\" words - filtering of [stop-list](https://en.wikipedia.org/wiki/Stop_word) expressions that are a valid part of the language, but introduce noise in the context of a given NLP task (e.g., articles, prepositions and even certain verbs and nouns in English are noise for most tasks based on the \"bag of words\" approach, where only statistical parameters of the text matter regardless of its explicit syntactic structure)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMpnWpX6SV5J"
   },
   "source": [
    "### Downloading a sample text\n",
    "\n",
    "![orwell](https://www.fi.muni.cz/~novacek/courses/pb016/labs/img/1984first.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xkjyNb3C0Xhr",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** The beginning of the \"raw version\" of the 1984 novel *****\n",
      "\n",
      "\n",
      "\n",
      "Project Gutenberg Australia\n",
      "\n",
      "\n",
      "\n",
      "Title: Nineteen eighty-four\n",
      "Author: George Orwell (pseudonym of Eric Blair) (1903-1950)\n",
      "* A Project Gutenberg of Australia eBook *\n",
      "eBook No.:  0100021.txt\n",
      "Language:   English\n",
      "Date first posted: August 2001\n",
      "Date most recently updated: November 2008\n",
      "\n",
      "Project Gutenberg of Australia eBooks are created from printed editions\n",
      "which are in the public domain in Australia, unless a copyright notice\n",
      "is included. We do NOT keep any eBooks in compliance with a particular\n",
      "paper edition.\n",
      "\n",
      "Copyright laws are changing all over the world. Be sure to check the\n",
      "copyright laws for your country before downloading or redistributing this\n",
      "file.\n",
      "\n",
      "This eBook is made available at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg of Australia License which may be viewed online at\n",
      "gutenberg.net.au/licence.html\n",
      "\n",
      "To contact Project Gutenberg of Australia go to gutenberg.net.au\n",
      "\n",
      "\n",
      "Title:      Nineteen eighty-four\n",
      "Author:     George Orwell (pseudonym of Eric Blair) (1903-1950)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PART ONE\n",
      "\n",
      "\n",
      "\n",
      "Chapter 1\n",
      "\n",
      "\n",
      "\n",
      "It was a bright cold day in April, and the clocks were striking thirteen.\n",
      "Winston Smith, his chin nuzzled into his breast in an effort to escape the\n",
      "vile wind, slipped quickly through the glass doors of Victory Mansions,\n",
      "though not quickly enough to prevent a swirl of gritty dust from entering\n",
      "along with him.\n",
      "\n",
      "The hallway smelt of boiled cabbage and old rag mats. At one end of it a\n",
      "coloured poster, too large for indoor display, had been tacked to the wall.\n",
      "It depicted simply an enormous face, more than a metre wide: the face of a\n",
      "man of about forty-five, with a heavy black moustache and ruggedly handsome\n",
      "features. Winston made for the stairs. It was no use trying the lift. Even\n",
      "at the best of times it was seldom working, and at present the electric\n",
      "current was cut off during daylight hours. It was part of the economy drive\n",
      "in preparation for Hate Week. The flat was seven flights up, and Winston,\n",
      "who was thirty-nine and had a varicose ulcer above his right ankle, went\n",
      "slowly, resting several times on the way. On each landing, opposite the\n",
      "lift-shaft, the poster with the enormous face gazed from the wall. It was\n",
      "one of those pictures which are so contrived that the eyes follow you about\n",
      "when you move. BIG BROTHER IS WATCHING YOU, the caption beneath it ran.\n",
      "\n",
      "Inside the flat a fruity voice was reading out a list of figures which had\n",
      "something to do with the production of pig-iron. The voice came from an\n",
      "oblong metal plaque like a dulled mirror which formed part of the surface\n",
      "of the right-hand wall. Winston turned a switch and the voice sank\n",
      "somewhat, though the words were still distinguishable. The instrument\n",
      "(the telescreen, it was called) could be dimmed, but there was no way of\n",
      "shutting it off completely. He moved over to the window: a smallish, frail\n",
      "figure, the meagreness of his body merely emphasized by the blue overalls\n",
      "which were the uniform of the party. His hair was very fair, his face\n",
      "naturally sanguine, his skin roughened by coarse soap and blunt razor\n",
      "blades and the cold of the winter that had just ended.\n",
      "\n",
      "Outside, even through the shut window-pane, the world looked cold. Down in\n",
      "the street little eddies of wind were whirling dust and torn paper into\n",
      "spirals, and though the sun was shining and the sky a harsh blue, there\n",
      "seemed to be no colour in anything, except the posters that were plastered\n",
      "everywhere. The black-moustachio'd face gazed down from every commanding\n",
      "corner. There was one on the house-front immediately opposite. BIG BROTHER\n",
      "IS WATCHING YOU, the caption said, while the dark eyes looked deep into\n",
      "Winston's own. Down at street level another poster, torn at one corner,\n",
      "flapped fitfully in the wind, alternately covering and uncovering the\n",
      "single word INGSOC. In the far distance a helicopter skimmed down between\n",
      "the roofs, hovered for an instant like a bluebottle, and darted away again\n",
      "with a curving flight. It was the police patrol, snooping into people's\n",
      "windows. The patrols did not matter, however. Only the Thought Police\n",
      "mattered.\n",
      "\n",
      "\n",
      "***** The end of the \"raw version\" of the 1984 novel *****\n",
      "\n",
      "VOCABULARY. The C vocabulary was supplementary to the others and\n",
      "consisted entirely of scientific and technical terms. These resembled the\n",
      "scientific terms in use today, and were constructed from the same roots,\n",
      "but the usual care was taken to define them rigidly and strip them of\n",
      "undesirable meanings. They followed the same grammatical rules as the\n",
      "words in the other two vocabularies. Very few of the C words had any\n",
      "currency either in everyday speech or in political speech. Any scientific\n",
      "worker or technician could find all the words he needed in the list devoted\n",
      "to his own speciality, but he seldom had more than a smattering of the\n",
      "words occurring in the other lists. Only a very few words were common to\n",
      "all lists, and there was no vocabulary expressing the function of Science\n",
      "as a habit of mind, or a method of thought, irrespective of its particular\n",
      "branches. There was, indeed, no word for 'Science', any meaning that it\n",
      "could possibly bear being already sufficiently covered by the word INGSOC.\n",
      "\n",
      "From the foregoing account it will be seen that in Newspeak the expression\n",
      "of unorthodox opinions, above a very low level, was well-nigh impossible.\n",
      "It was of course possible to utter heresies of a very crude kind, a\n",
      "species of blasphemy. It would have been possible, for example, to say\n",
      "BIG BROTHER IS UNGOOD. But this statement, which to an orthodox ear merely\n",
      "conveyed a self-evident absurdity, could not have been sustained by\n",
      "reasoned argument, because the necessary words were not available. Ideas\n",
      "inimical to Ingsoc could only be entertained in a vague wordless form,\n",
      "and could only be named in very broad terms which lumped together and\n",
      "condemned whole groups of heresies without defining them in doing so.\n",
      "One could, in fact, only use Newspeak for unorthodox purposes by\n",
      "illegitimately translating some of the words back into Oldspeak. For\n",
      "example, ALL MANS ARE EQUAL was a possible Newspeak sentence, but only\n",
      "in the same sense in which ALL MEN ARE REDHAIRED is a possible Oldspeak\n",
      "sentence. It did not contain a grammatical error, but it expressed\n",
      "a palpable untruth--i.e. that all men are of equal size, weight, or\n",
      "strength. The concept of political equality no longer existed, and this\n",
      "secondary meaning had accordingly been purged out of the word EQUAL.\n",
      "In 1984, when Oldspeak was still the normal means of communication,\n",
      "the danger theoretically existed that in using Newspeak words one might\n",
      "remember their original meanings. In practice it was not difficult for\n",
      "any person well grounded in DOUBLETHINK to avoid doing this, but within\n",
      "a couple of generations even the possibility of such a lapse would have\n",
      "vanished. A person growing up with Newspeak as his sole language would no\n",
      "more know that EQUAL had once had the secondary meaning of 'politically\n",
      "equal', or that FREE had once meant 'intellectually free', than for\n",
      "instance, a person who had never heard of chess would be aware of the\n",
      "secondary meanings attaching to QUEEN and ROOK. There would be many\n",
      "crimes and errors which it would be beyond his power to commit, simply\n",
      "because they were nameless and therefore unimaginable. And it was to be\n",
      "foreseen that with the passage of time the distinguishing characteristics\n",
      "of Newspeak would become more and more pronounced--its words growing\n",
      "fewer and fewer, their meanings more and more rigid, and the chance of\n",
      "putting them to improper uses always diminishing.\n",
      "\n",
      "When Oldspeak had been once and for all superseded, the last link with\n",
      "the past would have been severed. History had already been rewritten,\n",
      "but fragments of the literature of the past survived here and there,\n",
      "imperfectly censored, and so long as one retained one's knowledge of\n",
      "Oldspeak it was possible to read them. In the future such fragments, even\n",
      "if they chanced to survive, would be unintelligible and untranslatable.\n",
      "It was impossible to translate any passage of Oldspeak into Newspeak unless\n",
      "it either referred to some technical process or some very simple everyday\n",
      "action, or was already orthodox (GOODTHINKFUL would be the Newspeak\n",
      "expression) in tendency. In practice this meant that no book written before\n",
      "approximately 1960 could be translated as a whole. Pre-revolutionary\n",
      "literature could only be subjected to ideological translation--that is,\n",
      "alteration in sense as well as language. Take for example the well-known\n",
      "passage from the Declaration of Independence:\n",
      "\n",
      "\n",
      "WE HOLD THESE TRUTHS TO BE SELF-EVIDENT, THAT ALL MEN ARE CREATED EQUAL,\n",
      "THAT THEY ARE ENDOWED BY THEIR CREATOR WITH CERTAIN INALIENABLE RIGHTS,\n",
      "THAT AMONG THESE ARE LIFE, LIBERTY, AND THE PURSUIT OF HAPPINESS.\n",
      "THAT TO SECURE THESE RIGHTS, GOVERNMENTS ARE INSTITUTED AMONG MEN,\n",
      "DERIVING THEIR POWERS FROM THE CONSENT OF THE GOVERNED. THAT WHENEVER\n",
      "ANY FORM OF GOVERNMENT BECOMES DESTRUCTIVE OF THOSE ENDS, IT IS THE RIGHT\n",
      "OF THE PEOPLE TO ALTER OR ABOLISH IT, AND TO INSTITUTE NEW GOVERNMENT...\n",
      "\n",
      "\n",
      "It would have been quite impossible to render this into Newspeak while\n",
      "keeping to the sense of the original. The nearest one could come to doing\n",
      "so would be to swallow the whole passage up in the single word CRIMETHINK.\n",
      "A full translation could only be an ideological translation, whereby\n",
      "Jefferson's words would be changed into a panegyric on absolute government.\n",
      "\n",
      "A good deal of the literature of the past was, indeed, already being\n",
      "transformed in this way. Considerations of prestige made it desirable to\n",
      "preserve the memory of certain historical figures, while at the same time\n",
      "bringing their achievements into line with the philosophy of Ingsoc.\n",
      "Various writers, such as Shakespeare, Milton, Swift, Byron, Dickens, and\n",
      "some others were therefore in process of translation: when the task had\n",
      "been completed, their original writings, with all else that survived of\n",
      "the literature of the past, would be destroyed. These translations were\n",
      "a slow and difficult business, and it was not expected that they would\n",
      "be finished before the first or second decade of the twenty-first\n",
      "century. There were also large quantities of merely utilitarian\n",
      "literature--indispensable technical manuals, and the like--that had to\n",
      "be treated in the same way. It was chiefly in order to allow time for\n",
      "the preliminary work of translation that the final adoption of Newspeak\n",
      "had been fixed for so late a date as 2050.\n",
      "\n",
      "\n",
      "\n",
      "THE END\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Project Gutenberg Australia\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import library for opening URLs, etc.\n",
    "import urllib.request\n",
    "\n",
    "# open a link to sample text\n",
    "\n",
    "sample_text_link = \"http://gutenberg.net.au/ebooks01/0100021.txt\"\n",
    "f = urllib.request.urlopen(sample_text_link)\n",
    "\n",
    "# decoding the contents of the link (just convert the binary string to text -\n",
    "# it's already in a relatively clean plain text format)\n",
    "\n",
    "sample_text = f.read().decode(\"utf-8\")\n",
    "\n",
    "# print the beginning and ending of the text\n",
    "\n",
    "beginning = sample_text[:4115]\n",
    "ending = sample_text[-6315:]\n",
    "\n",
    "print('***** The beginning of the \"raw version\" of the 1984 novel *****\\n')\n",
    "print(beginning)\n",
    "\n",
    "print('\\n***** The end of the \"raw version\" of the 1984 novel *****\\n')\n",
    "print(ending)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbpYgDZsXTMX"
   },
   "source": [
    "### Cleaning the sample text\n",
    "- Removal of meta-data about the publication, appendices behind the story, other adjustments aimed at obtaining the text itself without structural annotations (i.e., annotation of parts, chapters, etc.).\n",
    "- Notes on the solution:\n",
    "  - The procedure is often very arbitrary, depending on the source text and what we want to do with it.\n",
    "  - A good start is to look at parts of the text, e.g., with `print(sample_text[:K])` and `print(sample_text[-K:])`, where `K` is a reasonably small number of characters (see above).\n",
    "  - From what we see, we decide what to delete, replace, etc.\n",
    "  - Substitutions using [regular expressions](https://en.wikipedia.org/wiki/Regular_expression) are often useful for text cleanup (without the use of specialized NLP libraries, but also with them). For details, see [re](https://docs.python.org/3/library/re.html) module in the standard Python library, specifically the `re.sub()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_JumLLSjX5U1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cleaner beginning of the 1984 novel *****\n",
      "\n",
      "It was a bright cold day in April, and the clocks were striking thirteen.\n",
      "Winston Smith, his chin nuzzled into his breast in an effort to escape the\n",
      "vile wind, slipped quickly through the glass doors of Victory Mansions,\n",
      "though not quickly enough to prevent a swirl of gritty dust from entering\n",
      "along with him.\n",
      "\n",
      "The hallway smelt of boiled cabbage and old rag mats. At one end of it a\n",
      "coloured poster, too large for indoor display, had been tacked to the wall.\n",
      "It depicted simply an enormous face, more than a metre wide: the face of a\n",
      "man of about forty-five, with a heavy black moustache and ruggedly handsome\n",
      "features. Winston made for the stairs. It was no use trying the lift. Even\n",
      "at the best of times it was seldom working, and at present the electric\n",
      "current was cut off during daylight hours. It was part of the economy drive\n",
      "in preparation for Hate Week. The flat was seven flights up, and Winston,\n",
      "who was thirty-nine and had a varicose ulcer above his right ankle, went\n",
      "slowly, resting several times on the way. On each landing, opposite the\n",
      "lift-shaft, the poster with the enormous face gazed from the wall. It was\n",
      "one of those pictures which are so contrived that the eyes follow you about\n",
      "when you move. BIG BROTHER IS WATCHING YOU, the caption beneath it ran.\n",
      "\n",
      "Inside the flat a fruity voice was reading out a list of figures which had\n",
      "something to do with the production of pig-iron. The voice came from an\n",
      "oblong metal plaque like a dulled mirror which formed part of the surface\n",
      "of the right-hand wall. Winston turned a switch and the voice sank\n",
      "somewhat, though the words were still distinguishable. The instrument\n",
      "(the telescreen, it was called) could be dimmed, but there was no way of\n",
      "shutting it off completely. He moved over to the window: a smallish, frail\n",
      "figure, the meagreness of his body merely emphasized by the blue overalls\n",
      "which were the uniform of the party. His hair was very fair, his face\n",
      "naturally sanguine, his skin roughened by coarse soap and blunt razor\n",
      "blades and the cold of the winter that had just ended.\n",
      "\n",
      "Outside, even through the shut window-pane, the world looked cold. Down in\n",
      "the street little eddies of wind were whirling dust and torn paper into\n",
      "spirals, and though the sun was shining and the sky a harsh blue, there\n",
      "seemed to be no colour in anything, except the posters that were plastered\n",
      "everywhere. The black-moustachio'd face gazed down from every commanding\n",
      "corner. There was one on the house-front immediately opposite. BIG BROTHER\n",
      "IS WATCHING YOU, the caption said, while the dark eyes looked deep into\n",
      "Winston's own. Down at street level another poster, torn at one corner,\n",
      "flapped fitfully in the wind, alternately covering and uncovering the\n",
      "single word INGSOC. In the far distance a helicopter skimmed down between\n",
      "the roofs, hovered for an instant like a bluebottle, and darted away again\n",
      "with a curving flight. It was the police patrol, snooping into people's\n",
      "windows. The patrols did not matter, however. Only the Thought Police\n",
      "mattered.\n",
      "\n",
      "\n",
      "***** Cleaner ending of the 1984 novel *****\n",
      "\n",
      "He gazed up at the enormous face. Forty years it had taken him to learn\n",
      "what kind of smile was hidden beneath the dark moustache. O cruel, needless\n",
      "misunderstanding! O stubborn, self-willed exile from the loving breast!\n",
      "Two gin-scented tears trickled down the sides of his nose. But it was all\n",
      "right, everything was all right, the struggle was finished. He had won\n",
      "the victory over himself. He loved Big Brother.\n"
     ]
    }
   ],
   "source": [
    "# cutting the metadata in the beginning\n",
    "\n",
    "cleaner_text = sample_text.split('PART ONE')[1]\n",
    "\n",
    "# cutting the appendix after the main story\n",
    "\n",
    "cleaner_text = cleaner_text.split('APPENDIX')[0]\n",
    "\n",
    "# deleting the '\\r' characters\n",
    "\n",
    "cleaner_text = cleaner_text.replace('\\r','')\n",
    "\n",
    "# removing structural annotations using the RE module\n",
    "\n",
    "import re\n",
    "\n",
    "cleaner_text = re.sub('PART [A-Z]+','',cleaner_text)\n",
    "cleaner_text = re.sub('Chapter [0-9]+','',cleaner_text)\n",
    "cleaner_text = re.sub('THE END','',cleaner_text)\n",
    "\n",
    "# cutting the whitespace around the text\n",
    "\n",
    "cleaner_text = cleaner_text.strip()\n",
    "\n",
    "# printing the beginning and ending\n",
    "\n",
    "cleaner_beginning = cleaner_text[:3010]\n",
    "cleaner_ending = cleaner_text[-412:]\n",
    "\n",
    "print('***** Cleaner beginning of the 1984 novel *****\\n')\n",
    "print(cleaner_beginning)\n",
    "\n",
    "print('\\n***** Cleaner ending of the 1984 novel *****\\n')\n",
    "print(cleaner_ending)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wuyr49GV3nId"
   },
   "source": [
    "---\n",
    "## 2. Tokenization, tagging and stemming (dummy pipeline)\n",
    "\n",
    "__Basic facts - tokenization__\n",
    "- Creating lists of individual paragraphs, sentences and words from the sample text.\n",
    "\n",
    "__Basic facts - tagging__\n",
    "- Assignment of grammatical or other categories to individual parts of the text.\n",
    "- One of the most common methods of tagging text is to assign corresponding part of speech tags to individual words (POS - part of speech tagging).\n",
    "- However, more complex units can also be tagged, e.g., noun and verb phrases, modifier phrases, etc., other, more abstract parts of syntactic trees, semantic tags, etc.\n",
    "\n",
    "__Basic facts - stemming (and lemmatisation)__\n",
    "- The process of reducing inflected (or sometimes derived) words to their word stem, base or root form, or some other sort of a canonical form.\n",
    " - Stemming is reduction of the word to a form that is most common among all its morphological variants.\n",
    " - Lemmatization is another (though often very related) form of normalisation by grouping together the inflected forms of a word so they can be analysed as a single canonical item that still bears the meaning of the original word - a lemma, or also a dictionary form.\n",
    "- An example in English: `wait, waits, waiting, ...` $\\rightarrow$ `wait` (stem), `wait` (lemma).\n",
    "- An example in Czech: `čekat, čeká, čekající, ...` $\\rightarrow$ `ček` (stem), `čekat` (lemma)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efgjiY6Ls7tZ"
   },
   "source": [
    "### Naive tokenization of the sample text\n",
    "- Notes on the solution:\n",
    "  - For paragraphs, one can take advantage of the fact that they are separated by double new lines. However, it might also be good to deal with the individual lines in the paragraphs themselves so that they become a uniform, unwrapped text.\n",
    "  - For splitting the text into sentences and words it is possible to use the punctuation marks or spaces, respecively, and the `split()` function (either from the standard Python library or from the `re` module)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Mj_jb6his7t6"
   },
   "outputs": [],
   "source": [
    "# function to create a paragraph list from the input text\n",
    "def paragraph_tokenizer(text,min_wlen=5):\n",
    "  \"\"\"Tokenizing the text to paragraphs.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  text : str\n",
    "      The input text to be tokenized.\n",
    "  min_wlen : int, optional (default is 5)\n",
    "      The minimum number of words in a paragraph for the paragraph\n",
    "      to be included in the tokenized output list.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  list\n",
    "      List of paragraph strings contained in the input text.\n",
    "  \"\"\"\n",
    "\n",
    "  splits = [x.replace('\\n', ' ') for x in text.split('\\n\\n')]\n",
    "  return [x for x in splits if len(x.split()) >= min_wlen]\n",
    "\n",
    "# function to create a sentence list from the input text\n",
    "def sentence_tokenizer(text):\n",
    "  \"\"\"Tokenizing the text to sentences.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  text : str\n",
    "      The input text to be tokenized.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  list\n",
    "      List of sentence strings contained in the input text.\n",
    "  \"\"\"\n",
    "\n",
    "  return re.split('[\\.?!]', text)\n",
    "\n",
    "# function to create a word list from the input text\n",
    "def word_tokenizer(text):\n",
    "  \"\"\"Tokenizing the text to words.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  text : str\n",
    "      The input text to be tokenized.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  list\n",
    "      List of word strings contained in the input text.\n",
    "  \"\"\"\n",
    "\n",
    "  return [x.strip('\\.,;!') for x in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WMlGb8uCSLyH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 4 paragraphs of 1984:\n",
      " ['It was a bright cold day in April, and the clocks were striking thirteen. Winston Smith, his chin nuzzled into his breast in an effort to escape the vile wind, slipped quickly through the glass doors of Victory Mansions, though not quickly enough to prevent a swirl of gritty dust from entering along with him.', 'The hallway smelt of boiled cabbage and old rag mats. At one end of it a coloured poster, too large for indoor display, had been tacked to the wall. It depicted simply an enormous face, more than a metre wide: the face of a man of about forty-five, with a heavy black moustache and ruggedly handsome features. Winston made for the stairs. It was no use trying the lift. Even at the best of times it was seldom working, and at present the electric current was cut off during daylight hours. It was part of the economy drive in preparation for Hate Week. The flat was seven flights up, and Winston, who was thirty-nine and had a varicose ulcer above his right ankle, went slowly, resting several times on the way. On each landing, opposite the lift-shaft, the poster with the enormous face gazed from the wall. It was one of those pictures which are so contrived that the eyes follow you about when you move. BIG BROTHER IS WATCHING YOU, the caption beneath it ran.', 'Inside the flat a fruity voice was reading out a list of figures which had something to do with the production of pig-iron. The voice came from an oblong metal plaque like a dulled mirror which formed part of the surface of the right-hand wall. Winston turned a switch and the voice sank somewhat, though the words were still distinguishable. The instrument (the telescreen, it was called) could be dimmed, but there was no way of shutting it off completely. He moved over to the window: a smallish, frail figure, the meagreness of his body merely emphasized by the blue overalls which were the uniform of the party. His hair was very fair, his face naturally sanguine, his skin roughened by coarse soap and blunt razor blades and the cold of the winter that had just ended.', \"Outside, even through the shut window-pane, the world looked cold. Down in the street little eddies of wind were whirling dust and torn paper into spirals, and though the sun was shining and the sky a harsh blue, there seemed to be no colour in anything, except the posters that were plastered everywhere. The black-moustachio'd face gazed down from every commanding corner. There was one on the house-front immediately opposite. BIG BROTHER IS WATCHING YOU, the caption said, while the dark eyes looked deep into Winston's own. Down at street level another poster, torn at one corner, flapped fitfully in the wind, alternately covering and uncovering the single word INGSOC. In the far distance a helicopter skimmed down between the roofs, hovered for an instant like a bluebottle, and darted away again with a curving flight. It was the police patrol, snooping into people's windows. The patrols did not matter, however. Only the Thought Police mattered.\"]\n",
      "\n",
      "The first 2 sentences of 1984:\n",
      " ['It was a bright cold day in April, and the clocks were striking thirteen', ' Winston Smith, his chin nuzzled into his breast in an effort to escape the vile wind, slipped quickly through the glass doors of Victory Mansions, though not quickly enough to prevent a swirl of gritty dust from entering along with him']\n",
      "\n",
      "The first 8 words of 1984:\n",
      " ['It', 'was', 'a', 'bright', 'cold', 'day', 'in', 'April']\n"
     ]
    }
   ],
   "source": [
    "# creating a list of paragraphs of the novel in 1984 and listing the first 4\n",
    "paragraphs = paragraph_tokenizer(cleaner_text)\n",
    "print('The first 4 paragraphs of 1984:\\n', paragraphs[:4])\n",
    "# creating a list of sentences of the novel in 1984 and listing the first 2\n",
    "sentences = []\n",
    "for paragraph in paragraphs:\n",
    "  sentences += sentence_tokenizer(paragraph)\n",
    "print('\\nThe first 2 sentences of 1984:\\n', sentences[:2])\n",
    "# creating a list of words of the novel in 1984 and listing the first 8\n",
    "words = []\n",
    "for sentence in sentences:\n",
    "  words += word_tokenizer(sentence)\n",
    "print('\\nThe first 8 words of 1984:\\n', words[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPrDqOGTzPlP"
   },
   "source": [
    "### Naive tagging and stemming of the sample text\n",
    "- Tagging the first sentence of the 1984 novel with symbols of the relevant parts of speech (POS).\n",
    "- Conseuqent stemming depending on the specific POS tags.\n",
    "- Notes on the solution:\n",
    "  - One can simply assign tags to individual words according to a hard-coded look-up dictionary (which is not very smart or scalable in practice, but it's OK for the purposes of this exercise).\n",
    "  - One can assume that the tags for each relevant word type are as follows: `DT` for articles (\"a\", \"an\", \"the\"),` NN` for nouns or numerals, `VB` for verbs, `PR` for pronouns, `JJ` for adjectives,` IN` for prepositions, `CC` for conjunctions, and` ?? `for unknown words.\n",
    "  - Stemming simply strips (some of) the word suffixes based on their POS tag then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "n7PcdjCw0frC"
   },
   "outputs": [],
   "source": [
    "# naive POS tagger\n",
    "\n",
    "def dummy_pos_tagger(words):\n",
    "  \"\"\"Adding POS tags to the list of input words.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  words : list\n",
    "      The input list of words to be tagged.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  list\n",
    "      List of `(word, POS_tag)` pairs.\n",
    "  \"\"\"\n",
    "\n",
    "  # list of tagged words\n",
    "  tagged_words = []\n",
    "  # look-up dictionary with tags assigned to the words of interest\n",
    "  tag_dict = {\n",
    "    'it' : 'PR',\n",
    "    'was' : 'VB',\n",
    "    'a' : 'DT',\n",
    "    'bright' : 'JJ',\n",
    "    'cold' : 'JJ',\n",
    "    'day' : 'NN',\n",
    "    'in' : 'IN',\n",
    "    'april' : 'NN',\n",
    "    'and' : 'CC',\n",
    "    'the' : 'DT',\n",
    "    'clocks' : 'NN',\n",
    "    'were' : 'VB',\n",
    "    'striking' : 'VB',\n",
    "    'thirteen' : 'NN'\n",
    "  }\n",
    "  # the tagging itself\n",
    "  for word in words:\n",
    "    tag = '??' # tag for unknown words\n",
    "    if word.lower() in tag_dict:\n",
    "      tag = tag_dict[word.lower()]\n",
    "    tagged_words.append((word,tag))\n",
    "  return tagged_words\n",
    "\n",
    "# naive stemmer\n",
    "\n",
    "def dummy_stemmer(tagged_words):\n",
    "  \"\"\"Adding stems tags to the list of input `(word, POS_tag)`\n",
    "  pairs.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  tagged_words : list\n",
    "      The input list of pairs containing words and their POS tags.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  list\n",
    "      List of the stems of the words in the input list.\n",
    "  \"\"\"\n",
    "\n",
    "  stemmed_words = []\n",
    "  for word, tag in tagged_words:\n",
    "    stemmed = word\n",
    "    if tag == 'NN':\n",
    "      stemmed = word.rstrip('s')\n",
    "    elif tag == 'VB':\n",
    "      if word.endswith('ed'):\n",
    "        stemmed = word.rstrip('ed')\n",
    "      if word.endswith('ing'):\n",
    "        stemmed = word.rstrip('ing')\n",
    "    stemmed_words.append(stemmed)\n",
    "  return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ZedyCWmnSatC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW WORDS   :\n",
      "It\n",
      "was\n",
      "a\n",
      "bright\n",
      "cold\n",
      "day\n",
      "in\n",
      "April\n",
      "and\n",
      "the\n",
      "clocks\n",
      "were\n",
      "striking\n",
      "thirteen\n",
      "\n",
      "TAGGED WORDS:\n",
      "('It', 'PR')\n",
      "('was', 'VB')\n",
      "('a', 'DT')\n",
      "('bright', 'JJ')\n",
      "('cold', 'JJ')\n",
      "('day', 'NN')\n",
      "('in', 'IN')\n",
      "('April', 'NN')\n",
      "('and', 'CC')\n",
      "('the', 'DT')\n",
      "('clocks', 'NN')\n",
      "('were', 'VB')\n",
      "('striking', 'VB')\n",
      "('thirteen', 'NN')\n",
      "\n",
      "STEMMED WORDS:\n",
      "It\n",
      "was\n",
      "a\n",
      "bright\n",
      "cold\n",
      "day\n",
      "in\n",
      "April\n",
      "and\n",
      "the\n",
      "clock\n",
      "were\n",
      "strik\n",
      "thirteen\n"
     ]
    }
   ],
   "source": [
    "# printing the words of the first sentence\n",
    "words = word_tokenizer(sentences[0])\n",
    "print('RAW WORDS   :\\n'+'\\n'.join(words))\n",
    "tagged_words = dummy_pos_tagger(words)\n",
    "# printing the tagged words of the first sentence\n",
    "print('\\nTAGGED WORDS:\\n'+'\\n'.join([str(x) for x in tagged_words]))\n",
    "stemmed_words = dummy_stemmer(tagged_words)\n",
    "# printing the stemmed words of the first sentence\n",
    "print('\\nSTEMMED WORDS:\\n'+'\\n'.join([str(x) for x in stemmed_words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3nUx95eVjuD"
   },
   "source": [
    "---\n",
    "\n",
    "## 3. The NLTK NLP library, shallow syntactic analysis (smarter pipeline)\n",
    "\n",
    "__Basic facts__\n",
    "- There are a number of mature tools for doing NLP.\n",
    "- One of the widely used libraries in Python is the Natural Language Toolkit - [NLTK](https://www.nltk.org/).\n",
    "- NLTK contains easy-to-use implementations of a number of state of the art techniques, algorithms, corpora, etc.\n",
    " - It can thus solve all the above tasks (tokenization, tagging, stemming), among other things, which you will experiment with yourselves.\n",
    "- Once these tasks are solved, one can focus on one of the \"holy grails\" of NLP - the syntactic analysis (also, parsing) of sentences in natural language.\n",
    "  - Essential for determining grammatical correctness of sentences and also a prerequisite to various methods of determining meaning of sentences in as broad range of contexts as possible (semantic and discourse analysis - arguably the so far rather unattainable ultimate goals of NLP).\n",
    "  - Parsing can be relatively [shallow](https://en.wikipedia.org/wiki/Shallow_parsing), or more complex, such as [dependency](https://en.wikipedia.org/wiki/Dependency_grammar) and/or [probabilistic](https://en.wikipedia.org/wiki/Statistical_parsing) (for more details, see the lecture and/or specialized NLP courses).\n",
    "  - However, in these labs we will only deal with the shallow one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RL0A3bXy8Co"
   },
   "source": [
    "### __Exercise 3.1: Tokenization, tagging and stemming using NLTK__\n",
    "- Implement a simple NLP pipeline solving the tasks presented in the previous sections using the [NLTK](https://www.nltk.org/) library (which will lead to a much more systematic and robust solution).\n",
    "- More specifically, check the NLTK docs and other relevant online materials to find out how to do the following:\n",
    " - Tokenize the cleaned text of the 1984 novel to sentences.\n",
    " - Tokenize the first sentence of the novel to words.\n",
    " - POS-tag the first sentence of the novel.\n",
    " - Stem the words in the first sentence of the novel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/jindmen/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jindmen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to /home/jindmen/nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/jindmen/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing NLTK\n",
    "import nltk\n",
    "import pycrfsuite\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "## Downloading NLTK used libraries\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('rslp')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "XW6xI6to4_gI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It was a bright cold day in April, and the clocks were striking thirteen.', 'Winston Smith, his chin nuzzled into his breast in an effort to escape the\\nvile wind, slipped quickly through the glass doors of Victory Mansions,\\nthough not quickly enough to prevent a swirl of gritty dust from entering\\nalong with him.']\n",
      "['It', 'was', 'a', 'bright', 'cold', 'day', 'in', 'April', ',', 'and', 'the', 'clocks', 'were', 'striking', 'thirteen', '.', 'Winston', 'Smith', ',', 'his']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vblagoje/bert-english-uncased-finetuned-pos were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagger Hunpos:\n",
      "[('It', b'PRP'), ('was', b'VBD'), ('a', b'DT'), ('bright', b'JJ'), ('cold', b'JJ'), ('day', b'NN'), ('in', b'IN'), ('April', b'NNP'), (',', b','), ('and', b'CC'), ('the', b'DT'), ('clocks', b'NNS'), ('were', b'VBD'), ('striking', b'VBG'), ('thirteen', b'NN'), ('.', b'.'), ('Winston', b'NNP'), ('Smith', b'NNP'), (',', b','), ('his', b'PRP$')]\n",
      "Tagger Perceptron:\n",
      "[('It', 'PRP'), ('was', 'VBD'), ('a', 'DT'), ('bright', 'JJ'), ('cold', 'JJ'), ('day', 'NN'), ('in', 'IN'), ('April', 'NNP'), (',', ','), ('and', 'CC'), ('the', 'DT'), ('clocks', 'NNS'), ('were', 'VBD'), ('striking', 'VBG'), ('thirteen', 'NN'), ('.', '.'), ('Winston', 'NNP'), ('Smith', 'NNP'), (',', ','), ('his', 'PRP$')]\n",
      "Tagger BERT:\n",
      "[('it', 'PRON'), ('was', 'VERB'), ('a', 'PROPN'), ('bright', 'ADJ'), ('cold', 'ADJ'), ('day', 'NOUN'), ('in', 'ADP'), ('april', 'PROPN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('the', 'DET'), ('clocks', 'NOUN'), ('were', 'VERB'), ('striking', 'VERB'), ('thirteen', 'NUM'), ('.', 'PUNCT'), ('winston', 'PROPN'), ('smith', 'PROPN'), (',', 'PUNCT'), ('his', 'PRON')]\n",
      "\n",
      "Analysis of dissimiliarity of taggers:\n",
      "\n",
      "Mismatch on word 28: to TO TO RP\n",
      "in an effort to escape the vile\n",
      "\n",
      "Mismatch on word 31: vile JJ NN JJ\n",
      "to escape the vile wind , slipped\n",
      "\n",
      "Mismatch on word 38: glass NN NN JJ\n",
      "quickly through the glass doors of Victory\n",
      "\n",
      "Mismatch on word 45: not RB RB RP\n",
      "Mansions , though not quickly enough to\n",
      "\n",
      "Mismatch on word 47: enough JJ RB RB\n",
      "though not quickly enough to prevent a\n",
      "\n",
      "Mismatch on word 48: to TO TO RP\n",
      "not quickly enough to prevent a swirl\n",
      "\n",
      "Mismatch on word 57: along RP RB IN\n",
      "dust from entering along with him .\n",
      "\n",
      "Mismatch on word 63: smelt VB NN VB\n",
      ". The hallway smelt of boiled cabbage\n",
      "\n",
      "Mismatch on word 65: boiled JJ JJ VB\n",
      "hallway smelt of boiled cabbage and old\n",
      "\n",
      "Mismatch on word 73: one CD CD NN\n",
      "mats . At one end of it\n",
      "\n",
      "Mismatch on word 78: coloured VB JJ JJ\n",
      "of it a coloured poster , too\n",
      "\n",
      "Mismatch on word 90: to TO TO IN\n",
      "had been tacked to the wall .\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nAll three models are mostly the same. The difference between them\\nis not that big. The sets of tags are very different between NLTK and BERT.\\nThis difference may have introduced part of the difference after mapping.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## NLTK pipeline\n",
    "# paragraph tokenize\n",
    "# sentence tokenize\n",
    "sentences = nltk.tokenize.sent_tokenize(cleaner_text)\n",
    "# word tokenize\n",
    "words = nltk.tokenize.word_tokenize(cleaner_text)\n",
    "print(sentences[:2])\n",
    "print(words[:20])\n",
    "\n",
    "\"\"\"\n",
    "POS Tagging\n",
    "\n",
    "Hunpos tagger uses trigrams (3-word sequences) for its tagging.\n",
    "Perceptron tagger uses AI.\n",
    "BERT is a family of language models allowing much more than just POS tagging.\n",
    "\n",
    "In all three I use the pre-trained version.\n",
    "\"\"\"\n",
    "\n",
    "hunpos_tagger = nltk.tag.hunpos.HunposTagger('en_wsj.model').tag\n",
    "perceptron_tagger = nltk.tag.perceptron.PerceptronTagger().tag\n",
    "bert_tagger = pipeline(\"token-classification\", model=\"vblagoje/bert-english-uncased-finetuned-pos\")\n",
    "\n",
    "taggers = [\n",
    "    (hunpos_tagger, 'Hunpos'),\n",
    "    (perceptron_tagger, 'Perceptron'),\n",
    "    (lambda x: list(map(lambda z: (z[0]['word'], z[0]['entity']), bert_tagger(x))), 'BERT')\n",
    "]\n",
    "\n",
    "for tagger, name in taggers:\n",
    "    print(f\"Tagger {name}:\")\n",
    "    print(tagger(words[:20]))\n",
    "\n",
    "'''\n",
    "Mapping of both BERT model and NLTK models to closer\n",
    "(and smaller) space of tags for analysis.\n",
    "'''\n",
    "bert_mapping = {\n",
    "    'ADJ': 'JJ',\n",
    "    'ADV': 'RB',\n",
    "    'ADP': 'IN',\n",
    "    'AUX': 'VB',\n",
    "    'CCONJ': 'CC',\n",
    "    'DET': 'DT',\n",
    "    'INTJ': 'UH',\n",
    "    'NOUN': 'NN',\n",
    "    'NUM': 'NN',\n",
    "    'PART': 'RP',\n",
    "    'PRON': 'PRP',\n",
    "    'PROPN': 'NN',\n",
    "    'PUNCT': 'PUNCT',\n",
    "    'SCONJ': 'IN',\n",
    "    'SYM': 'X',\n",
    "    'VERB': 'VB',\n",
    "    'X': 'X',\n",
    "}\n",
    "\n",
    "others_mapping = {\n",
    "    ',': 'PUNCT',\n",
    "    '.': 'PUNCT',\n",
    "    'NNS': 'NN',\n",
    "    'NNP': 'NN',\n",
    "    'NNPS': 'NN',\n",
    "    'VBP': 'VB',\n",
    "    'VBN': 'VB',\n",
    "    'VBG': 'VB',\n",
    "    'VBD': 'VB',\n",
    "    'VBZ': 'VB',\n",
    "    'PRP$': 'PRP',\n",
    "    'RBS': 'RB',\n",
    "    'RBR': 'RB',\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Analysis of similiarity of taggers:\n",
    "Generates tags for both these taggers, then compare them.\n",
    "Where taggers differ, print neighborhood of the differing word.\n",
    "\"\"\"\n",
    "size = 100\n",
    "padding = 3\n",
    "\n",
    "def generate_tags(tagger, words, modifier=lambda x: x[1]):\n",
    "    return map(modifier, tagger(words))\n",
    "\n",
    "def generate_tags_from_sentences(tagger, sentences, size=200, modifier=lambda x: x[1],\n",
    "                                 filter_func=lambda x: True):\n",
    "    tags = []\n",
    "    sentidx = 0\n",
    "    while len(tags) < size and sentidx < len(sentences):\n",
    "        tags.extend(filter(filter_func, tagger(sentences[sentidx])))\n",
    "        sentidx += 1\n",
    "    tags = tags[:size]\n",
    "    tags = map(modifier, tags)\n",
    "    return tags\n",
    "\n",
    "def map_tag(tag, mapping):\n",
    "    if tag in mapping:\n",
    "        return mapping[tag]\n",
    "    return tag\n",
    "\n",
    "tags_hunpos = map(\n",
    "    lambda x: map_tag(x, others_mapping),\n",
    "    generate_tags(taggers[0][0], words[:size], lambda x: x[1].decode('utf-8'))\n",
    ")\n",
    "tags_perceptron = map(\n",
    "    lambda x: map_tag(x, others_mapping),\n",
    "    generate_tags(taggers[1][0], words[:size])\n",
    ")\n",
    "tags_bert = map(\n",
    "    lambda x: map_tag(x, bert_mapping),\n",
    "    generate_tags_from_sentences(bert_tagger, sentences, size,\n",
    "                                 modifier=lambda x: x['entity'],\n",
    "                                 filter_func=lambda x: len(x['word']) == 0 or x['word'][0] != '#'\n",
    "                                )\n",
    ")\n",
    "\n",
    "print('\\nAnalysis of dissimiliarity of taggers:\\n')\n",
    "for i, (tag1, tag2, tag3) in enumerate(zip(tags_hunpos, tags_perceptron, tags_bert)):\n",
    "    if tag1 != tag2 or tag2 != tag3:\n",
    "        print(f\"Mismatch on word {i}: {words[i]} {tag1} {tag2} {tag3}\")\n",
    "        print(' '.join(words[max(0, i-padding):i+padding+1]))\n",
    "        print()\n",
    "\n",
    "'''\n",
    "All three models are mostly the same. The difference between them\n",
    "is not that big. The sets of tags are very different between NLTK and BERT.\n",
    "This difference may have introduced part of the difference after mapping.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmer WordNet:\n",
      "['It', 'wa', 'a', 'bright', 'cold', 'day', 'in', 'April', ',', 'and', 'the', 'clock', 'were', 'striking', 'thirteen', '.', 'Winston', 'Smith', ',', 'his']\n",
      "Stemmer Lancaster:\n",
      "['it', 'was', 'a', 'bright', 'cold', 'day', 'in', 'april', ',', 'and', 'the', 'clock', 'wer', 'striking', 'thirteen', '.', 'winston', 'smi', ',', 'his']\n",
      "Stemmer Porter:\n",
      "['it', 'wa', 'a', 'bright', 'cold', 'day', 'in', 'april', ',', 'and', 'the', 'clock', 'were', 'strike', 'thirteen', '.', 'winston', 'smith', ',', 'hi']\n",
      "Stemmer RegexpStemmer:\n",
      "['It', 'wa', 'a', 'bright', 'cold', 'day', 'in', 'April', ',', 'and', 'th', 'clock', 'wer', 'strik', 'thirteen', '.', 'Winston', 'Smith', ',', 'hi']\n",
      "Stemmer RSLP:\n",
      "['it', 'wa', 'a', 'bright', 'cold', 'day', 'in', 'april', ',', 'and', 'the', 'clock', 'wer', 'striking', 'thirteen', '.', 'winston', 'smith', ',', 'hi']\n",
      "Stemmer Snowball English:\n",
      "['it', 'was', 'a', 'bright', 'cold', 'day', 'in', 'april', ',', 'and', 'the', 'clock', 'were', 'strike', 'thirteen', '.', 'winston', 'smith', ',', 'his']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nEach stemmer does something a little bit different.\\nOne difference is in the size of letters, WordNet and Regexp allow for capital letters by default.\\nAnother difference is in the stemming of verbs. Some do the stemming of verbs into infinitive\\nform, others rip off suffixes.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming\n",
    "stemmers = [\n",
    "    (nltk.stem.wordnet.WordNetLemmatizer().lemmatize, 'WordNet'),\n",
    "    (nltk.stem.lancaster.LancasterStemmer().stem, 'Lancaster'),\n",
    "    (nltk.stem.porter.PorterStemmer().stem, 'Porter'),\n",
    "    (nltk.stem.regexp.RegexpStemmer('ing$|s$|e$|able$').stem, 'RegexpStemmer'),\n",
    "    (nltk.stem.rslp.RSLPStemmer().stem, 'RSLP'),\n",
    "    (nltk.stem.snowball.EnglishStemmer().stem, 'Snowball English')\n",
    "]\n",
    "\n",
    "for stemmer, name in stemmers:\n",
    "    print(f\"Stemmer {name}:\")\n",
    "    print(list(map(lambda x: stemmer(x), words[:20])))\n",
    "\n",
    "'''\n",
    "Each stemmer does something a little bit different.\n",
    "One difference is in the size of letters, WordNet and Regexp allow for capital letters by default.\n",
    "Another difference is in the stemming of verbs. Some do the stemming of verbs into infinitive\n",
    "form, others rip off suffixes.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOlRu54okuMS"
   },
   "source": [
    "### __Exercise 3.2: Shallow parsing using NLTK__\n",
    "- Use the list of tagged words from the previous example to create a shallow syntactic tree for the first sentence of the 1984 novel.\n",
    "- In this tree, deal primarily with simple noun and verb phrases, but you may also deal with other, more complex syntactic structures if you will.\n",
    "- Notes on the solution:\n",
    " - NLTK can do most of the work for you (search for the corresponding docs or StackOverflow questions).\n",
    " - For example, the class `RegexpParser` can come in handy (as included in the pre-filled code cell below). It only needs a grammar, which defines individual syntactic groups (noun phrases, verb phrases, etc.) based on the sequence of POS tags of the respective words contained in them.\n",
    " - A grammar is simply a string defining rules for regular expression analysis (see pre-defined code below).\n",
    " - E.g., a sequence of two or more adjectives would match the following rule: `JJ2: {<JJ><JJ>+}`.\n",
    " - You can assume that noun phrases typically consist of nouns (`<NN.*>`) and their modification by adjectives (`<JJ>`) with the possible use of articles (`<DT>`), while verb phrases consist of some unbroken sequence of verb forms (`<VB.*>`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "k2LaEg3Co89p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('it', 'PRON'), ('was', 'AUX'), ('a', 'DET'), ('bright', 'ADJ'), ('cold', 'ADJ'), ('day', 'NOUN'), ('in', 'ADP'), ('april', 'PROPN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('the', 'DET'), ('clocks', 'NOUN'), ('were', 'AUX'), ('striking', 'VERB'), ('thirteen', 'NUM'), ('.', 'PUNCT')]\n",
      "A shallow parse tree of the first sentence of 1984:\n",
      "(S\n",
      "  (SENT\n",
      "    (NP it/PRON)\n",
      "    (VP\n",
      "      was/AUX\n",
      "      (NPD\n",
      "        a/DET\n",
      "        (NP\n",
      "          (NP bright/ADJ (NP cold/ADJ (NP day/NOUN)))\n",
      "          (ADV in/ADP (NP april/PROPN))))))\n",
      "  (CCONJ ,/PUNCT and/CCONJ)\n",
      "  (SENT\n",
      "    (NPD the/DET (NP clocks/NOUN))\n",
      "    (VP were/AUX striking/VERB (NP thirteen/NUM)))\n",
      "  (CCONJ ./PUNCT))\n"
     ]
    }
   ],
   "source": [
    "# TODO - complete the grammar for shallow parsing\n",
    "'''\n",
    "This is a readable grammar for the BERT model.\n",
    "Also see the RegexpParser. As the parser works using regexes,\n",
    "it needs multiple passes (loop) to compute.\n",
    "(Basically it needs up to one cycle for each layer of syntactic tree.)\n",
    "'''\n",
    "chunk_grammar = \"\"\"\n",
    "  NP: {<PRON|NOUN|PROPN|NUM>}\n",
    "  NP: {<ADJ><NP>}\n",
    "  NP: {<NP><ADV>}\n",
    "  ADV: {<ADP><NP>}\n",
    "  NPD: {<DET><NP>}\n",
    "  VP: {<VERB>|<AUX><VERB>?<NP|NPD>}\n",
    "  CCONJ: {<PUNCT><CCONJ>?}\n",
    "  SENT: {<NP|NPD><VP>}\n",
    "  S: {<SENT><CCONJ><S>|<SENT><PUNCT>}\n",
    "  NP: {<NP><CCONJ><SENT><CCONJ>}\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "I have also tried parsing sentences other than the first one,\n",
    "but as the RegexpParser does not allow backtracking, I have failed to do so.\n",
    "'''\n",
    "\n",
    "def remove_split_words(words):\n",
    "    # The BERT model sometimes splits some words with the second part\n",
    "    # starting with '##', this function counteracts this behavior\n",
    "    no_splits = []\n",
    "    last_word = words[0]\n",
    "    for word, tag in words[1:]:\n",
    "        assert len(word) >= 1\n",
    "        if word[:2] == '##':\n",
    "            last_word = (last_word[0] + word[2:], tag)\n",
    "        else:\n",
    "            no_splits.append(last_word)\n",
    "            last_word = (word, tag)\n",
    "    no_splits.append(last_word)\n",
    "    return no_splits\n",
    "\n",
    "# the actual syntactic analysis\n",
    "tagged_words = list(map(\n",
    "    lambda x: (x['word'], x['entity']),\n",
    "    bert_tagger(sentences[0])\n",
    "))\n",
    "tagged_words = remove_split_words(tagged_words)\n",
    "chunk_parser = nltk.RegexpParser(chunk_grammar, root_label='S', loop=20)\n",
    "chunked = chunk_parser.parse(tagged_words)\n",
    "\n",
    "# printing the syntactic tree\n",
    "print(tagged_words)\n",
    "print('A shallow parse tree of the first sentence of 1984:')\n",
    "print(chunked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGdY7oEfWQaf"
   },
   "source": [
    "---\n",
    "\n",
    "## 4. Sentiment analysis\n",
    "\n",
    "__Basic facts__\n",
    "- Analysis of emotional polarity of texts, or possibly multimodal data (reviews, news, political debates, social networks, etc.).\n",
    "- A technique with a number of practical applications (product development, marketing, market analysis and forecasting, public opinion mining, disease outbreak detection and management, crime detection, etc.).\n",
    "\n",
    "__Your task__\n",
    "- Split into groups (min 2, max 4 people).\n",
    "- Check the NLTK documentation, StackOverflow, etc., to find out which NLTK module(s) can be used for sentiment analysis and how they work. Feel free to use other sentiment analysis tools, though, should you find anything else that feels more appropriate and/or convenient.\n",
    "- Choose the approach that works best for you and try to answer the following questions:\n",
    "  - What is the overall tone of the 1984 novel? Is it a glorious utopia, or rather a gloomy dystopia?\n",
    "  - What is the most cheerful sentence (and optionally also a paragraph) of the novel?\n",
    "  - What is the most desperate sentence (and optionally also a paragraph) of the novel?\n",
    "- Take your time, play around (no need to have a complete solution - what's important is to search for possible approaches, play and learn). Feel free to ask for help anytime.\n",
    "- Discuss your analysis of the 1984 novel, and your general observations with the lab tutor. The collaborating members of the group with the best relative results and/or an interesting/elegant/efficient/unusual solution can earn bonus points.\n",
    "\n",
    "__Optional task__\n",
    "- Try to solve the same task using a [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) (a state of the art deep learning architecture, especially for NLP tasks).\n",
    " - You may get an off-the-shelf model, such as a [HuggingFace](https://huggingface.co/transformers/) one, as described for instance [here](https://satish1v.medium.com/sentiment-analysis-with-hugging-face-4b080d0cf34d).\n",
    " - You may also try to train (or rather [fine-tune](https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning))) your own transformer-based model, as described for instance [here](https://towardsdatascience.com/sentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671) or [here](https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/).\n",
    "- Compare the results with the \"classical\" sentiment analysis method you implemented before - which one works better, and why (or rather: does such a question even make sense at all)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "zJCfQf-fp3dv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "# TODO - analysis of the sentiment of the novel 1984\n",
    "#        (the actual solution is totally up to you)\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as vader_analyzer\n",
    "from transformers import pipeline\n",
    "\n",
    "'''\n",
    "I have used two models, Vader provided by NLTK and DistilBERT the default model\n",
    "for sentiment analysis provided by HuggingFace.\n",
    "\n",
    "As can be seen in the output, while Vader does yield \"vectors\" of the sentiment,\n",
    "that is how much positive, negative or neutral the sentence feels,\n",
    "DistilBERT only classifies the input sentence into either `POSITIVE` or `NEGATIVE`,\n",
    "giving how certain it is with its decision.\n",
    "The output of Vader can thus be used as is, the output of DistilBERT should be\n",
    "taken with a grain of salt, as we cannot sort the sentences on anything other\n",
    "than the certainity. This should however still be quite good measure.\n",
    "'''\n",
    "\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "vader = vader_analyzer().polarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the analysis of sentiment using transformer...\n",
      "Done\n",
      "Computing the analysis of sentiment using Vader...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing the analysis of sentiment using transformer...\")\n",
    "sentiment = list(map(lambda x: (x, classifier(x)), sentences))\n",
    "print(\"Done\")\n",
    "print(\"Computing the analysis of sentiment using Vader...\")\n",
    "sentiment_vader = list(map(lambda x: (x, vader(x)), sentences))\n",
    "print(\"Done\")\n",
    "\n",
    "positive = list(filter(lambda x: x[1][0]['label'] == 'POSITIVE', sentiment))\n",
    "negative = list(filter(lambda x: x[1][0]['label'] == 'NEGATIVE', sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good.\n",
      "{'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4404}\n",
      "\n",
      "Good.\n",
      "{'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4404}\n",
      "\n",
      "Thanks, comrade!'\n",
      "{'neg': 0.0, 'neu': 0.239, 'pos': 0.761, 'compound': 0.4926}\n",
      "\n",
      "Yes, YOU!\n",
      "{'neg': 0.0, 'neu': 0.251, 'pos': 0.749, 'compound': 0.4574}\n",
      "\n",
      "And, yes!\n",
      "{'neg': 0.0, 'neu': 0.251, 'pos': 0.749, 'compound': 0.4574}\n",
      "\n",
      "'Thass funny.\n",
      "{'neg': 0.0, 'neu': 0.256, 'pos': 0.744, 'compound': 0.4404}\n",
      "\n",
      "It showed respect, like.\n",
      "{'neg': 0.0, 'neu': 0.263, 'pos': 0.737, 'compound': 0.6808}\n",
      "\n",
      "Yes, I would.'\n",
      "{'neg': 0.0, 'neu': 0.27, 'pos': 0.73, 'compound': 0.4019}\n",
      "\n",
      "Yes, even...\n",
      "{'neg': 0.0, 'neu': 0.27, 'pos': 0.73, 'compound': 0.4019}\n",
      "\n",
      "'The truth, please, Winston.\n",
      "{'neg': 0.0, 'neu': 0.303, 'pos': 0.697, 'compound': 0.5574}\n",
      "\n",
      "A riot!\n",
      "{'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.5983}\n",
      "\n",
      "Damn!\n",
      "{'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.4574}\n",
      "\n",
      "O cruel, needless misunderstanding!\n",
      "{'neg': 0.873, 'neu': 0.127, 'pos': 0.0, 'compound': -0.784}\n",
      "\n",
      "He was blind, helpless, mindless.\n",
      "{'neg': 0.811, 'neu': 0.189, 'pos': 0.0, 'compound': -0.8225}\n",
      "\n",
      "There were no more doubts, no more arguments, no more pain, no more fear.\n",
      "{'neg': 0.781, 'neu': 0.219, 'pos': 0.0, 'compound': -0.9605}\n",
      "\n",
      "But no!\n",
      "{'neg': 0.756, 'neu': 0.244, 'pos': 0.0, 'compound': -0.4753}\n",
      "\n",
      "sobbed Winston.\n",
      "{'neg': 0.744, 'neu': 0.256, 'pos': 0.0, 'compound': -0.4404}\n",
      "\n",
      "But he fought furiously against his panic.\n",
      "{'neg': 0.738, 'neu': 0.262, 'pos': 0.0, 'compound': -0.9052}\n",
      "\n",
      "he cried.\n",
      "{'neg': 0.722, 'neu': 0.278, 'pos': 0.0, 'compound': -0.3818}\n",
      "\n",
      "he cried.\n",
      "{'neg': 0.722, 'neu': 0.278, 'pos': 0.0, 'compound': -0.3818}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using Vader:\n",
    "\n",
    "positive_sorted = sorted(sentiment_vader, key=lambda x: x[1]['pos'], reverse=True)\n",
    "negative_sorted = sorted(sentiment_vader, key=lambda x: x[1]['neg'], reverse=True)\n",
    "\n",
    "for sentence, label in positive_sorted[:10]:\n",
    "    print(sentence.replace('\\n', ' '))\n",
    "    print(label)\n",
    "    print()\n",
    "\n",
    "for sentence, label in negative_sorted[:10]:\n",
    "    print(sentence.replace('\\n', ' '))\n",
    "    print(label)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have glorious news for you.\n",
      "[{'label': 'POSITIVE', 'score': 0.9998831748962402}]\n",
      "\n",
      "The taste was delightful.\n",
      "[{'label': 'POSITIVE', 'score': 0.9998797178268433}]\n",
      "\n",
      "It's fascinating.'\n",
      "[{'label': 'POSITIVE', 'score': 0.9998770952224731}]\n",
      "\n",
      "Don't you like feeling: This is me, this is my hand, this is my leg, I'm real, I'm solid, I'm alive!\n",
      "[{'label': 'POSITIVE', 'score': 0.9998766183853149}]\n",
      "\n",
      "In fact I'm proud of her.\n",
      "[{'label': 'POSITIVE', 'score': 0.9998745918273926}]\n",
      "\n",
      "His body was healthy and strong.\n",
      "[{'label': 'POSITIVE', 'score': 0.999874472618103}]\n",
      "\n",
      "'Yes, perfectly.'\n",
      "[{'label': 'POSITIVE', 'score': 0.9998735189437866}]\n",
      "\n",
      "'It is a beautiful thing,' said the other appreciatively.\n",
      "[{'label': 'POSITIVE', 'score': 0.9998729228973389}]\n",
      "\n",
      "'She's beautiful,' he murmured.\n",
      "[{'label': 'POSITIVE', 'score': 0.999871015548706}]\n",
      "\n",
      "I'm good at games.\n",
      "[{'label': 'POSITIVE', 'score': 0.9998704195022583}]\n",
      "\n",
      "'Nonsense.\n",
      "[{'label': 'NEGATIVE', 'score': 0.9998021721839905}]\n",
      "\n",
      "'Nonsense.\n",
      "[{'label': 'NEGATIVE', 'score': 0.9998021721839905}]\n",
      "\n",
      "What a bore!\n",
      "[{'label': 'NEGATIVE', 'score': 0.9997996687889099}]\n",
      "\n",
      "Dust.\n",
      "[{'label': 'NEGATIVE', 'score': 0.9997977614402771}]\n",
      "\n",
      "It was not interesting.\n",
      "[{'label': 'NEGATIVE', 'score': 0.9997976422309875}]\n",
      "\n",
      "'The worst thing in the world,' said O'Brien, 'varies from individual to individual.\n",
      "[{'label': 'NEGATIVE', 'score': 0.9997939467430115}]\n",
      "\n",
      "It is written down.'\n",
      "[{'label': 'NEGATIVE', 'score': 0.9997932314872742}]\n",
      "\n",
      "A heavy black volume, amateurishly bound, with no name or title on the cover.\n",
      "[{'label': 'NEGATIVE', 'score': 0.9997929930686951}]\n",
      "\n",
      "'It's a ruin now.\n",
      "[{'label': 'NEGATIVE', 'score': 0.9997923970222473}]\n",
      "\n",
      "It was a miserable outfit.\n",
      "[{'label': 'NEGATIVE', 'score': 0.9997907280921936}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using transformer:\n",
    "\n",
    "positive_sorted = sorted(positive, key=lambda x: x[1][0]['score'], reverse=True)\n",
    "negative_sorted = sorted(negative, key=lambda x: x[1][0]['score'], reverse=True)\n",
    "\n",
    "for sentence, label in positive_sorted[:10]:\n",
    "    print(sentence.replace('\\n', ' '))\n",
    "    print(label)\n",
    "    print()\n",
    "\n",
    "for sentence, label in negative_sorted[:10]:\n",
    "    print(sentence.replace('\\n', ' '))\n",
    "    print(label)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOcmk4AaRaN8"
   },
   "source": [
    "---\n",
    "\n",
    "#### _Final note_ - the materials used in this notebook are adapted from works licensed by the original authors as follows:\n",
    "- Picture of the first edition of the novel 1984:\n",
    "  - Retrieved from Wikipedia [here](https://en.wikipedia.org/wiki/File:1984first.jpg)\n",
    "  - Author: [Brown University Library](http://library.brown.edu/search/c?SEARCH=PR6029.R8+N49+1949b)\n",
    "  - License: none, or rather [Public Domain](https://en.wikipedia.org/wiki/public_domain) in the US, but probably still copyrighted in the country of origin (UK)\n",
    "- The novel itself:\n",
    " - Retrieved from the Australian [Project Gutenberg](https://www.gutenberg.org/) site [here](http://gutenberg.net.au/ebooks01/0100021.txt)\n",
    " - Author: [George Orwell](https://en.wikipedia.org/wiki/George_Orwell)\n",
    " - License: [Public Domain](https://en.wikipedia.org/wiki/public_domain) in Australia and possibly in other jurisdictions, but in general, the copyright is held by the George Orwell estate and the text should be treated accordingly in terms of its public or any other use"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "py311_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
