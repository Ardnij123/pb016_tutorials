{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8Cj_X1VnmC0"
   },
   "source": [
    "# PB016: Artificial Intelligence I, labs 10 - Natural language processing\n",
    "\n",
    "This week's topic is natural language processing (NLP). We'll focus namely on:\n",
    "1. __Text acquisition and pre-processing__\n",
    "2. __Tokenization, tagging and stemming (dummy pipeline)__\n",
    "3. __The NLTK NLP library, shallow syntactic analysis (smarter pipeline)__\n",
    "4. __Sentiment analysis__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjLOiihWnmC1"
   },
   "source": [
    "---\n",
    "\n",
    "## 1. Text acquisition and preprocessing\n",
    "\n",
    "__Basic facts__\n",
    "- Usually a necessary step before applying the natural language processing methods themselves.\n",
    "- It consists mainly of preparing texts for machine processing (e.g., removal of OCR noise, markup, splitting in segments, normalisation, etc.).\n",
    "\n",
    "__Examples of typical tasks__\n",
    "- Conversion and cleaning of text - obtaining material (e.g., by downloading), encoding transformations, conversion from the original format to plain text, possible removal of noise in the form of OCR errors, formatting and other marks, annotation passages, etc.\n",
    "- Removal of \"irrelevant\" words - filtering of [stop-list](https://en.wikipedia.org/wiki/Stop_word) expressions that are a valid part of the language, but introduce noise in the context of a given NLP task (e.g., articles, prepositions and even certain verbs and nouns in English are noise for most tasks based on the \"bag of words\" approach, where only statistical parameters of the text matter regardless of its explicit syntactic structure)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMpnWpX6SV5J"
   },
   "source": [
    "### Downloading a sample text\n",
    "\n",
    "![orwell](https://www.fi.muni.cz/~novacek/courses/pb016/labs/img/1984first.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xkjyNb3C0Xhr"
   },
   "outputs": [],
   "source": [
    "# import library for opening URLs, etc.\n",
    "import urllib.request\n",
    "\n",
    "# open a link to sample text\n",
    "\n",
    "sample_text_link = \"http://gutenberg.net.au/ebooks01/0100021.txt\"\n",
    "f = urllib.request.urlopen(sample_text_link)\n",
    "\n",
    "# decoding the contents of the link (just convert the binary string to text -\n",
    "# it's already in a relatively clean plain text format)\n",
    "\n",
    "sample_text = f.read().decode(\"utf-8\")\n",
    "\n",
    "# print the beginning and ending of the text\n",
    "\n",
    "beginning = sample_text[:4115]\n",
    "ending = sample_text[-6315:]\n",
    "\n",
    "print('***** The beginning of the \"raw version\" of the 1984 novel *****\\n')\n",
    "print(beginning)\n",
    "\n",
    "print('\\n***** The end of the \"raw version\" of the 1984 novel *****\\n')\n",
    "print(ending)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbpYgDZsXTMX"
   },
   "source": [
    "### Cleaning the sample text\n",
    "- Removal of meta-data about the publication, appendices behind the story, other adjustments aimed at obtaining the text itself without structural annotations (i.e., annotation of parts, chapters, etc.).\n",
    "- Notes on the solution:\n",
    "  - The procedure is often very arbitrary, depending on the source text and what we want to do with it.\n",
    "  - A good start is to look at parts of the text, e.g., with `print(sample_text[:K])` and `print(sample_text[-K:])`, where `K` is a reasonably small number of characters (see above).\n",
    "  - From what we see, we decide what to delete, replace, etc.\n",
    "  - Substitutions using [regular expressions](https://en.wikipedia.org/wiki/Regular_expression) are often useful for text cleanup (without the use of specialized NLP libraries, but also with them). For details, see [re](https://docs.python.org/3/library/re.html) module in the standard Python library, specifically the `re.sub()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_JumLLSjX5U1"
   },
   "outputs": [],
   "source": [
    "# cutting the metadata in the beginning\n",
    "\n",
    "cleaner_text = sample_text.split('PART ONE')[1]\n",
    "\n",
    "# cutting the appendix after the main story\n",
    "\n",
    "cleaner_text = cleaner_text.split('APPENDIX')[0]\n",
    "\n",
    "# deleting the '\\r' characters\n",
    "\n",
    "cleaner_text = cleaner_text.replace('\\r','')\n",
    "\n",
    "# removing structural annotations using the RE module\n",
    "\n",
    "import re\n",
    "\n",
    "cleaner_text = re.sub('PART [A-Z]+','',cleaner_text)\n",
    "cleaner_text = re.sub('Chapter [0-9]+','',cleaner_text)\n",
    "cleaner_text = re.sub('THE END','',cleaner_text)\n",
    "\n",
    "# cutting the whitespace around the text\n",
    "\n",
    "cleaner_text = cleaner_text.strip()\n",
    "\n",
    "# printing the beginning and ending\n",
    "\n",
    "cleaner_beginning = cleaner_text[:3010]\n",
    "cleaner_ending = cleaner_text[-412:]\n",
    "\n",
    "print('***** Cleaner beginning of the 1984 novel *****\\n')\n",
    "print(cleaner_beginning)\n",
    "\n",
    "print('\\n***** Cleaner ending of the 1984 novel *****\\n')\n",
    "print(cleaner_ending)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wuyr49GV3nId"
   },
   "source": [
    "---\n",
    "## 2. Tokenization, tagging and stemming (dummy pipeline)\n",
    "\n",
    "__Basic facts - tokenization__\n",
    "- Creating lists of individual paragraphs, sentences and words from the sample text.\n",
    "\n",
    "__Basic facts - tagging__\n",
    "- Assignment of grammatical or other categories to individual parts of the text.\n",
    "- One of the most common methods of tagging text is to assign corresponding part of speech tags to individual words (POS - part of speech tagging).\n",
    "- However, more complex units can also be tagged, e.g., noun and verb phrases, modifier phrases, etc., other, more abstract parts of syntactic trees, semantic tags, etc.\n",
    "\n",
    "__Basic facts - stemming (and lemmatisation)__\n",
    "- The process of reducing inflected (or sometimes derived) words to their word stem, base or root form, or some other sort of a canonical form.\n",
    " - Stemming is reduction of the word to a form that is most common among all its morphological variants.\n",
    " - Lemmatization is another (though often very related) form of normalisation by grouping together the inflected forms of a word so they can be analysed as a single canonical item that still bears the meaning of the original word - a lemma, or also a dictionary form.\n",
    "- An example in English: `wait, waits, waiting, ...` $\\rightarrow$ `wait` (stem), `wait` (lemma).\n",
    "- An example in Czech: `čekat, čeká, čekající, ...` $\\rightarrow$ `ček` (stem), `čekat` (lemma)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efgjiY6Ls7tZ"
   },
   "source": [
    "### Naive tokenization of the sample text\n",
    "- Notes on the solution:\n",
    "  - For paragraphs, one can take advantage of the fact that they are separated by double new lines. However, it might also be good to deal with the individual lines in the paragraphs themselves so that they become a uniform, unwrapped text.\n",
    "  - For splitting the text into sentences and words it is possible to use the punctuation marks or spaces, respecively, and the `split()` function (either from the standard Python library or from the `re` module)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Mj_jb6his7t6"
   },
   "outputs": [],
   "source": [
    "# function to create a paragraph list from the input text\n",
    "def paragraph_tokenizer(text,min_wlen=5):\n",
    "  \"\"\"Tokenizing the text to paragraphs.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  text : str\n",
    "      The input text to be tokenized.\n",
    "  min_wlen : int, optional (default is 5)\n",
    "      The minimum number of words in a paragraph for the paragraph\n",
    "      to be included in the tokenized output list.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  list\n",
    "      List of paragraph strings contained in the input text.\n",
    "  \"\"\"\n",
    "\n",
    "  splits = [x.replace('\\n', ' ') for x in text.split('\\n\\n')]\n",
    "  return [x for x in splits if len(x.split()) >= min_wlen]\n",
    "\n",
    "# function to create a sentence list from the input text\n",
    "def sentence_tokenizer(text):\n",
    "  \"\"\"Tokenizing the text to sentences.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  text : str\n",
    "      The input text to be tokenized.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  list\n",
    "      List of sentence strings contained in the input text.\n",
    "  \"\"\"\n",
    "\n",
    "  return re.split('[\\.?!]', text)\n",
    "\n",
    "# function to create a word list from the input text\n",
    "def word_tokenizer(text):\n",
    "  \"\"\"Tokenizing the text to words.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  text : str\n",
    "      The input text to be tokenized.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  list\n",
    "      List of word strings contained in the input text.\n",
    "  \"\"\"\n",
    "\n",
    "  return [x.strip('\\.,;!') for x in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntaB3PhPSDnA"
   },
   "outputs": [],
   "source": [
    "# creating a list of paragraphs of the novel in 1984 and listing the first 4\n",
    "paragraphs = paragraph_tokenizer(cleaner_text)\n",
    "print('The first 4 paragraphs of 1984:\\n', paragraphs[:4])\n",
    "# creating a list of sentences of the novel in 1984 and listing the first 2\n",
    "sentences = []\n",
    "for paragraph in paragraphs:\n",
    "  sentences += sentence_tokenizer(paragraph)\n",
    "print('\\nThe first 2 sentences of 1984:\\n', sentences[:2])\n",
    "# creating a list of words of the novel in 1984 and listing the first 8\n",
    "words = []\n",
    "for sentence in sentences:\n",
    "  words += word_tokenizer(sentence)\n",
    "print('\\nThe first 8 words of 1984:\\n', words[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPrDqOGTzPlP"
   },
   "source": [
    "### Naive tagging and stemming of the sample text\n",
    "- Tagging the first sentence of the 1984 novel with symbols of the relevant parts of speech (POS).\n",
    "- Conseuqent stemming depending on the specific POS tags.\n",
    "- Notes on the solution:\n",
    "  - One can simply assign tags to individual words according to a hard-coded look-up dictionary (which is not very smart or scalable in practice, but it's OK for the purposes of this exercise).\n",
    "  - One can assume that the tags for each relevant word type are as follows: `DT` for articles (\"a\", \"an\", \"the\"),` NN` for nouns or numerals, `VB` for verbs, `PR` for pronouns, `JJ` for adjectives,` IN` for prepositions, `CC` for conjunctions, and` ?? `for unknown words.\n",
    "  - Stemming simply strips (some of) the word suffixes based on their POS tag then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "n7PcdjCw0frC"
   },
   "outputs": [],
   "source": [
    "# naive POS tagger\n",
    "\n",
    "def dummy_pos_tagger(words):\n",
    "  \"\"\"Adding POS tags to the list of input words.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  words : list\n",
    "      The input list of words to be tagged.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  list\n",
    "      List of `(word, POS_tag)` pairs.\n",
    "  \"\"\"\n",
    "\n",
    "  # list of tagged words\n",
    "  tagged_words = []\n",
    "  # look-up dictionary with tags assigned to the words of interest\n",
    "  tag_dict = {\n",
    "    'it' : 'PR',\n",
    "    'was' : 'VB',\n",
    "    'a' : 'DT',\n",
    "    'bright' : 'JJ',\n",
    "    'cold' : 'JJ',\n",
    "    'day' : 'NN',\n",
    "    'in' : 'IN',\n",
    "    'april' : 'NN',\n",
    "    'and' : 'CC',\n",
    "    'the' : 'DT',\n",
    "    'clocks' : 'NN',\n",
    "    'were' : 'VB',\n",
    "    'striking' : 'VB',\n",
    "    'thirteen' : 'NN'\n",
    "  }\n",
    "  # the tagging itself\n",
    "  for word in words:\n",
    "    tag = '??' # tag for unknown words\n",
    "    if word.lower() in tag_dict:\n",
    "      tag = tag_dict[word.lower()]\n",
    "    tagged_words.append((word,tag))\n",
    "  return tagged_words\n",
    "\n",
    "# naive stemmer\n",
    "\n",
    "def dummy_stemmer(tagged_words):\n",
    "  \"\"\"Adding stems tags to the list of input `(word, POS_tag)`\n",
    "  pairs.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  tagged_words : list\n",
    "      The input list of pairs containing words and their POS tags.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  list\n",
    "      List of the stems of the words in the input list.\n",
    "  \"\"\"\n",
    "\n",
    "  stemmed_words = []\n",
    "  for word, tag in tagged_words:\n",
    "    stemmed = word\n",
    "    if tag == 'NN':\n",
    "      stemmed = word.rstrip('s')\n",
    "    elif tag == 'VB':\n",
    "      if word.endswith('ed'):\n",
    "        stemmed = word.rstrip('ed')\n",
    "      if word.endswith('ing'):\n",
    "        stemmed = word.rstrip('ing')\n",
    "    stemmed_words.append(stemmed)\n",
    "  return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grqvAv1DSTt0"
   },
   "outputs": [],
   "source": [
    "# printing the words of the first sentence\n",
    "words = word_tokenizer(sentences[0])\n",
    "print('RAW WORDS   :\\n'+'\\n'.join(words))\n",
    "tagged_words = dummy_pos_tagger(words)\n",
    "# printing the tagged words of the first sentence\n",
    "print('\\nTAGGED WORDS:\\n'+'\\n'.join([str(x) for x in tagged_words]))\n",
    "stemmed_words = dummy_stemmer(tagged_words)\n",
    "# printing the stemmed words of the first sentence\n",
    "print('\\nSTEMMED WORDS:\\n'+'\\n'.join([str(x) for x in stemmed_words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3nUx95eVjuD"
   },
   "source": [
    "---\n",
    "\n",
    "## 3. The NLTK NLP library, shallow syntactic analysis (smarter pipeline)\n",
    "\n",
    "__Basic facts__\n",
    "- There are a number of mature tools for doing NLP.\n",
    "- One of the widely used libraries in Python is the Natural Language Toolkit - [NLTK](https://www.nltk.org/).\n",
    "- NLTK contains easy-to-use implementations of a number of state of the art techniques, algorithms, corpora, etc.\n",
    " - It can thus solve all the above tasks (tokenization, tagging, stemming), among other things, which you will experiment with yourselves.\n",
    "- Once these tasks are solved, one can focus on one of the \"holy grails\" of NLP - the syntactic analysis (also, parsing) of sentences in natural language.\n",
    "  - Essential for determining grammatical correctness of sentences and also a prerequisite to various methods of determining meaning of sentences in as broad range of contexts as possible (semantic and discourse analysis - arguably the so far rather unattainable ultimate goals of NLP).\n",
    "  - Parsing can be relatively [shallow](https://en.wikipedia.org/wiki/Shallow_parsing), or more complex, such as [dependency](https://en.wikipedia.org/wiki/Dependency_grammar) and/or [probabilistic](https://en.wikipedia.org/wiki/Statistical_parsing) (for more details, see the lecture and/or specialized NLP courses).\n",
    "  - However, in these labs we will only deal with the shallow one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RL0A3bXy8Co"
   },
   "source": [
    "### __Exercise 3.1: Tokenization, tagging and stemming using NLTK__\n",
    "- Implement a simple NLP pipeline solving the tasks presented in the previous sections using the [NLTK](https://www.nltk.org/) library (which will lead to a much more systematic and robust solution).\n",
    "- More specifically, check the NLTK docs and other relevant online materials to find out how to do the following:\n",
    " - Tokenize the cleaned text of the 1984 novel to sentences.\n",
    " - Tokenize the first sentence of the novel to words.\n",
    " - POS-tag the first sentence of the novel.\n",
    " - Stem the words in the first sentence of the novel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "XW6xI6to4_gI"
   },
   "outputs": [],
   "source": [
    "# importing NLTK\n",
    "import nltk\n",
    "\n",
    "# TODO - YOUR OWN SOLUTION GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHWoJfqUz4PN"
   },
   "source": [
    "### __Possible way of implementing the tokenization, tagging and stemming using NLTK__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tvcICD2_0FL8"
   },
   "outputs": [],
   "source": [
    "# importing the necessary NLTK modules\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# downloading the necessary NLTK data\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "# tokenization of all cleaned text to sentences\n",
    "sentences = nltk.sent_tokenize(cleaner_text)\n",
    "# tokenization the first sentence to words\n",
    "first_words = nltk.word_tokenize(sentences[0])\n",
    "# POS tagging of the first sentence\n",
    "tagged_words = nltk.pos_tag(first_words)\n",
    "# stemming the words in the first sentence\n",
    "stemmer = PorterStemmer()\n",
    "tagged_stemmed_words = [(x,y,stemmer.stem(x)) for x, y in tagged_words]\n",
    "\n",
    "# printing the tagged and stemmed sentence\n",
    "print('\\nThe first sentence of the novel 1984, POS-tagged and stemmed:')\n",
    "print('\\n'.join(['  '+str(x) for x in tagged_stemmed_words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOlRu54okuMS"
   },
   "source": [
    "### __Exercise 3.2: Shallow parsing using NLTK__\n",
    "- Use the list of tagged words from the previous example to create a shallow syntactic tree for the first sentence of the 1984 novel.\n",
    "- In this tree, deal primarily with simple noun and verb phrases, but you may also deal with other, more complex syntactic structures if you will.\n",
    "- Notes on the solution:\n",
    " - NLTK can do most of the work for you (search for the corresponding docs or StackOverflow questions).\n",
    " - For example, the class `RegexpParser` can come in handy (as included in the pre-filled code cell below). It only needs a grammar, which defines individual syntactic groups (noun phrases, verb phrases, etc.) based on the sequence of POS tags of the respective words contained in them.\n",
    " - A grammar is simply a string defining rules for regular expression analysis (see pre-defined code below).\n",
    " - E.g., a sequence of two or more adjectives would match the following rule: `JJ2: {<JJ><JJ>+}`.\n",
    " - You can assume that noun phrases typically consist of nouns (`<NN.*>`) and their modification by adjectives (`<JJ>`) with the possible use of articles (`<DT>`), while verb phrases consist of some unbroken sequence of verb forms (`<VB.*>`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWvFZLGglAa4"
   },
   "source": [
    "### __Possible approach to shallow parsing using NLTK__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wdgeas9IkfO1"
   },
   "outputs": [],
   "source": [
    "# the grammar for shallow parsing\n",
    "chunk_grammar = \"\"\"\n",
    "  NP: {<DT>?<JJ>*<NN.*>}\n",
    "  VP: {<VB.*>+}\n",
    "\"\"\"\n",
    "\n",
    "# the actual syntactic analysis\n",
    "chunk_parser = nltk.RegexpParser(chunk_grammar)\n",
    "chunked = chunk_parser.parse(tagged_words)\n",
    "\n",
    "# printing the syntactic tree\n",
    "print('A shallow parse tree of the first sentence of 1984:')\n",
    "print(chunked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGdY7oEfWQaf"
   },
   "source": [
    "---\n",
    "\n",
    "## 4. Sentiment analysis\n",
    "\n",
    "__Basic facts__\n",
    "- Analysis of emotional polarity of texts, or possibly multimodal data (reviews, news, political debates, social networks, etc.).\n",
    "- A technique with a number of practical applications (product development, marketing, market analysis and forecasting, public opinion mining, disease outbreak detection and management, crime detection, etc.).\n",
    "\n",
    "__Your task__\n",
    "- Split into groups (min 2, max 4 people).\n",
    "- Check the NLTK documentation, StackOverflow, etc., to find out which NLTK module(s) can be used for sentiment analysis and how they work. Feel free to use other sentiment analysis tools, though, should you find anything else that feels more appropriate and/or convenient.\n",
    "- Choose the approach that works best for you and try to answer the following questions:\n",
    " - What is the overall tone of the 1984 novel? Is it a glorious utopia, or rather a gloomy dystopia?\n",
    " - What is the most cheerful sentence (and optionally also a paragraph) of the novel?\n",
    " - What is the most desperate sentence (and optionally also a paragraph) of the novel?\n",
    "- Take your time, play around (no need to have a complete solution - what's important is to search for possible approaches, play and learn). Feel free to ask for help anytime.\n",
    "- Discuss your analysis of the 1984 novel, and your general observations with the lab tutor. The collaborating members of the group with the best relative results and/or an interesting/elegant/efficient/unusual solution can earn bonus points.\n",
    "\n",
    "__Optional task__\n",
    "- Try to solve the same task using a [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) (a state of the art deep learning architecture, especially for NLP tasks).\n",
    " - You may get an off-the-shelf model, such as a [HuggingFace](https://huggingface.co/transformers/) one, as described for instance [here](https://satish1v.medium.com/sentiment-analysis-with-hugging-face-4b080d0cf34d).\n",
    " - You may also try to train (or rather [fine-tune](https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning))) your own transformer-based model, as described for instance [here](https://towardsdatascience.com/sentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671) or [here](https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/).\n",
    "- Compare the results with the \"classical\" sentiment analysis method you implemented before - which one works better, and why (or rather: does such a question even make sense at all)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zJCfQf-fp3dv"
   },
   "outputs": [],
   "source": [
    "# TODO - analysis of the sentiment of the novel 1984\n",
    "#        (the actual totally up to you)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ein7Ntv3nmC3"
   },
   "source": [
    "### __Possible approach to sentiment analysis of the 1984 novel (using NLTK)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oefka46buX1X"
   },
   "source": [
    "#### Importing a pre-trained sentiment analysis module, downloading its data, initialising the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n5K3FEn8nmC3"
   },
   "outputs": [],
   "source": [
    "# import and download of the required classes and data (Valence Aware\n",
    "# Dictionary and sEntiment Reasoner)\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# initialising the analyser\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvM6sJ49ukNG"
   },
   "source": [
    "#### Getting the overall sentiment, using the whole text as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-shwe83upLO"
   },
   "outputs": [],
   "source": [
    "# analysis of the whole text at once and listing of relevant polarities\n",
    "polarity_overall = analyzer.polarity_scores(cleaner_text)\n",
    "print('\\nOverall polarity of the 1984 novel (entire text in one go):')\n",
    "print(' ', polarity_overall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0HVE8aburrA"
   },
   "source": [
    "#### Getting the most extreme sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VcSqX6bFutyX"
   },
   "outputs": [],
   "source": [
    "# variables for continuous updating of the most extreme sentences\n",
    "most_neg, most_pos = 1, -1\n",
    "extreme_sentences = {\n",
    "    'pos' : {'text' : None, 'polarity' : None},\n",
    "    'neg' : {'text' : None, 'polarity' : None}\n",
    "}\n",
    "# analysis of the whole text by sentences, calculating the average polarity of\n",
    "# all sentences\n",
    "avg_polarity = {'neg': 0, 'neu': 0, 'pos': 0, 'compound': 0}\n",
    "for sen in sentences:\n",
    "  polarity = analyzer.polarity_scores(sen)\n",
    "  avg_polarity['neg'] += polarity['neg']\n",
    "  avg_polarity['pos'] += polarity['pos']\n",
    "  avg_polarity['neu'] += polarity['neu']\n",
    "  avg_polarity['compound'] += polarity['compound']\n",
    "  if polarity['compound'] > most_pos:\n",
    "    extreme_sentences['pos']['text'] = sen.replace('\\n',' ')\n",
    "    extreme_sentences['pos']['polarity'] = polarity\n",
    "    most_pos = polarity['compound']\n",
    "  if polarity['compound'] < most_neg:\n",
    "    extreme_sentences['neg']['text'] = sen.replace('\\n',' ')\n",
    "    extreme_sentences['neg']['polarity'] = polarity\n",
    "    most_neg = polarity['compound']\n",
    "for key in avg_polarity:\n",
    "  avg_polarity[key] /= len(sentences)\n",
    "\n",
    "# listing of the overall polarity and the most extreme sentences\n",
    "print('Overall polarity of the 1984 novel (average of sentence polarities):')\n",
    "print(' ', avg_polarity)\n",
    "print('\\nThe most positive sentence of the 1984 novel:')\n",
    "print(' ', extreme_sentences['pos']['text'])\n",
    "print('  ... sentence polarity:', extreme_sentences['pos']['polarity'])\n",
    "print('\\nThe most negative sentence of the 1984 novel:')\n",
    "print(' ', extreme_sentences['neg']['text'])\n",
    "print('  ... sentence polarity:', extreme_sentences['neg']['polarity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcJThKDnuw_u"
   },
   "source": [
    "#### Getting the most extreme paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ep96o802uzN_"
   },
   "outputs": [],
   "source": [
    "# variables for continuous updating of the most extreme paragraphs\n",
    "most_neg, most_pos = 1, -1\n",
    "extreme_paragraphs = {\n",
    "    'pos' : {'text' : None, 'polarity' : None},\n",
    "    'neg' : {'text' : None, 'polarity' : None}\n",
    "}\n",
    "# analysis of individual paragraphs (could also be done using the averages of\n",
    "# polarity of individual sentences)\n",
    "avg_polarity = {'neg': 0, 'neu': 0, 'pos': 0, 'compound': 0}\n",
    "for par in paragraphs:\n",
    "  polarity = analyzer.polarity_scores(par)\n",
    "  avg_polarity['neg'] += polarity['neg']\n",
    "  avg_polarity['pos'] += polarity['pos']\n",
    "  avg_polarity['neu'] += polarity['neu']\n",
    "  avg_polarity['compound'] += polarity['compound']\n",
    "  if polarity['compound'] > most_pos:\n",
    "    extreme_paragraphs['pos']['text'] = par\n",
    "    extreme_paragraphs['pos']['polarity'] = polarity\n",
    "    most_pos = polarity['compound']\n",
    "  if polarity['compound'] < most_neg:\n",
    "    extreme_paragraphs['neg']['text'] = par\n",
    "    extreme_paragraphs['neg']['polarity'] = polarity\n",
    "    most_neg = polarity['compound']\n",
    "for key in avg_polarity:\n",
    "  avg_polarity[key] /= len(paragraphs)\n",
    "\n",
    "# listing of the overall polarity and the most extreme paragraphs\n",
    "print('Overall polarity of the 1984 novel (average of paragraph polarities):')\n",
    "print(' ', avg_polarity)\n",
    "print('\\nThe most positive paragraph of the 1984 novel:')\n",
    "print(' ', extreme_paragraphs['pos']['text'])\n",
    "print('  ... paragraph polarity:', extreme_paragraphs['pos']['polarity'])\n",
    "print('\\nThe most negative paragraph of the 1984 novel:')\n",
    "print(' ', extreme_paragraphs['neg']['text'])\n",
    "print('  ... paragraph polarity:', extreme_paragraphs['neg']['polarity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfTNun567lBm"
   },
   "source": [
    "### __Possible approach to sentiment analysis of the 1984 novel (using a transformer)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "us6GBbu4OyAe"
   },
   "source": [
    "#### Installing transformers (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6TEkoJ-O0b2"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhzNSUJKO10D"
   },
   "source": [
    "#### Importing and initialising a sentiment analysis pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1XfZbcLcO7iP"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# initialising the sentiment analysis pipeline using a specific BERT-based\n",
    "# model and GPU\n",
    "classifier = pipeline(\"sentiment-analysis\",\n",
    "                      model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "                      device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUHNI6HHPW2O"
   },
   "source": [
    "#### Function for getting the two most extreme items from a list of chunks, and the average sentiment of the whole list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "-ye_AmYAPZkT"
   },
   "outputs": [],
   "source": [
    "def get_extremes(chunks,model,max_tokens=512):\n",
    "  \"\"\"Getting the highest positive and negative polarities of the input list\n",
    "  of chunks.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  chunks : list\n",
    "      The input list of text chunks (e.g., sentences or paragraphs) for which\n",
    "      the sentiment polarities are to be computed.\n",
    "  model : transformers.pipeline\n",
    "      A pretrained HuggingFace classifier performing the sentiment analysis.\n",
    "  max_tokens : int, optional (default is 512)\n",
    "      The maximum number of tokens allowed in a chunk.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  tuple\n",
    "      A pair of the following variables:\n",
    "      - extreme_chunks (dict) - a dictionary storing the texts and polarities\n",
    "        of the chunks with the highest positive and negative polarities.\n",
    "      - overall (float) - mean polarity of the input list of chunks.\n",
    "  \"\"\"\n",
    "\n",
    "  # initialising bookkeeping variables\n",
    "  most_neg, most_pos, overall = 0, 0, 0\n",
    "  extreme_chunks = {\n",
    "    'pos' : {'text' : None, 'polarity' : None},\n",
    "    'neg' : {'text' : None, 'polarity' : None}\n",
    "  }\n",
    "\n",
    "  # processing the chunks in the input list\n",
    "  for chunk in chunks:\n",
    "    approx_tokens = chunk.split()\n",
    "    if len(approx_tokens) > max_tokens:\n",
    "      print('WARNING - skipping a too long chunk:',\n",
    "            chunk)\n",
    "      continue\n",
    "    try:\n",
    "      polarity = model(chunk)[0]\n",
    "    except RuntimeError:\n",
    "      print('WARNING - RuntimeError (chunk too long, probably; omitting):',\n",
    "            chunk)\n",
    "      continue\n",
    "    label, score = polarity['label'], polarity['score']\n",
    "    if label == 'POSITIVE':\n",
    "      overall += score\n",
    "      if score > most_pos:\n",
    "        most_pos = score\n",
    "        extreme_chunks['pos']['text'] = chunk\n",
    "        extreme_chunks['pos']['polarity'] = score\n",
    "    elif label == 'NEGATIVE':\n",
    "      overall -= score\n",
    "      if score > most_neg:\n",
    "        most_neg = score\n",
    "        extreme_chunks['neg']['text'] = chunk\n",
    "        extreme_chunks['neg']['polarity'] = score\n",
    "  overall /= len(chunks)\n",
    "\n",
    "  # returning the extreme chunks and the overall score of the text\n",
    "  return extreme_chunks, overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnpVACsrVrFP"
   },
   "source": [
    "#### Getting extreme sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6FUTNJtWVtFN"
   },
   "outputs": [],
   "source": [
    "# computing the extreme sentences\n",
    "extreme_sentences, avg_polarity = get_extremes(sentences,classifier)\n",
    "\n",
    "# listing of the overall polarity and the most extreme sentences\n",
    "print('Overall polarity of the 1984 novel (mean sentence polarities):')\n",
    "print(' ', avg_polarity)\n",
    "print('\\nThe most positive sentence of the 1984 novel:')\n",
    "print(' ', extreme_sentences['pos']['text'])\n",
    "print('  ... sentence polarity:', extreme_sentences['pos']['polarity'])\n",
    "print('\\nThe most negative sentence of the 1984 novel:')\n",
    "print(' ', extreme_sentences['neg']['text'])\n",
    "print('  ... sentence polarity:', -extreme_sentences['neg']['polarity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruqLDlEZWX-b"
   },
   "source": [
    "#### Getting extreme paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twAQkeNGWaPO"
   },
   "outputs": [],
   "source": [
    "# computing the extreme sentences\n",
    "extreme_paragraphs, avg_polarity = get_extremes(paragraphs,classifier)\n",
    "\n",
    "# listing of the overall polarity and the most extreme paragraphs\n",
    "print('Overall polarity of the 1984 novel (mean paragraph polarities):')\n",
    "print(' ', avg_polarity)\n",
    "print('\\nThe most positive paragraph of the 1984 novel:')\n",
    "print(' ', extreme_paragraphs['pos']['text'])\n",
    "print('  ... paragraph polarity:', extreme_paragraphs['pos']['polarity'])\n",
    "print('\\nThe most negative paragraph of the 1984 novel:')\n",
    "print(' ', extreme_paragraphs['neg']['text'])\n",
    "print('  ... paragraph polarity:', -extreme_paragraphs['neg']['polarity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "yLjsBHRLnmDa",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "---\n",
    "\n",
    "#### _Final note_ - the materials used in this notebook are adapted from works licensed by the original authors as follows:\n",
    "- Picture of the first edition of the novel 1984:\n",
    "  - Retrieved from Wikipedia [here](https://en.wikipedia.org/wiki/File:1984first.jpg)\n",
    "  - Author: [Brown University Library](http://library.brown.edu/search/c?SEARCH=PR6029.R8+N49+1949b)\n",
    "  - License: none, or rather [Public Domain](https://en.wikipedia.org/wiki/public_domain) in the US, but probably still copyrighted in the country of origin (UK)\n",
    "- The novel itself:\n",
    " - Retrieved from the Australian [Project Gutenberg](https://www.gutenberg.org/) site [here](http://gutenberg.net.au/ebooks01/0100021.txt)\n",
    " - Author: [George Orwell](https://en.wikipedia.org/wiki/George_Orwell)\n",
    " - License: [Public Domain](https://en.wikipedia.org/wiki/public_domain) in Australia and possibly in other jurisdictions, but in general, the copyright is held by the George Orwell estate and the text should be treated accordingly in terms of its public or any other use"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
