\documentclass{beamer}

% chktex-file 1
% chktex-file 8
% chktex-file 9
% chktex-file 10

\usetheme[workplace=fi]{MU}
\usepackage[utf8]{inputenc}
\usepackage[
  main=czech,
  english
]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{csquotes}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{multicol}
\usepackage{listings}

\newcommand{\RR}{\ensuremath{\mathbb{R}}}

\usepackage{tikz}
\usetikzlibrary{arrows, shapes.geometric}

\newlength{\nodesize}
\setlength{\nodesize}{1.5em}
\tikzset{and/.style = {shape=circle,draw,minimum size=\nodesize}}
\tikzset{or/.style = {shape=rectangle,draw,minimum size=\nodesize}}

\tikzset{min/.style = {shape=circle,draw,minimum size=\nodesize}}
\tikzset{max/.style = {shape=rectangle,draw,minimum size=\nodesize}}
\tikzset{rand/.style = {diamond,draw,minimum size=\nodesize}}

\tikzset{edge/.style = {->,> = latex'}}

% Allows for split by delimiter
% \getNth{string}{delimiter}{entry_no}
% Source: https://tex.stackexchange.com/questions/12810/how-do-i-split-a-string
\ExplSyntaxOn
\NewDocumentCommand{\getNth}{mmm}
  {
    % #1 string, #2 separator, #3 index
    \seq_set_split:Nnx \l_tmpa_seq { #2 } { #1 }
    \seq_item:Nn \l_tmpa_seq { #3 }
  }
\ExplSyntaxOff

\setbeamertemplate{enumerate item}{\alph{enumi})}  % chktex 9 chktex 10

\newcommand{\tictactoe}[2]{%
    \draw (#1.center) ++(-.5,-.5) -- +(0,1);
    \draw (#1.center) ++(-.1667,-.5) -- +(0,1);
    \draw (#1.center) ++(.1667,-.5) -- +(0,1);
    \draw (#1.center) ++(.5,-.5) -- +(0,1);
    \draw (#1.center) ++(-.5,-.5) -- +(1,0);
    \draw (#1.center) ++(-.5,-.1667) -- +(1,0);
    \draw (#1.center) ++(-.5,.1667) -- +(1,0);
    \draw (#1.center) ++(-.5,.5) -- +(1,0);
    \draw (#1.center) ++(-.3333,.3333) node {\getNth{#2}{;}{1}};
    \draw (#1.center) ++(0,.3333) node {\getNth{#2}{;}{2}};
    \draw (#1.center) ++(.3333,.3333) node {\getNth{#2}{;}{3}};
    \draw (#1.center) ++(-.3333,0) node {\getNth{#2}{;}{4}};
    \draw (#1.center) ++(0,0) node {\getNth{#2}{;}{5}};
    \draw (#1.center) ++(.3333,0) node {\getNth{#2}{;}{6}};
    \draw (#1.center) ++(-.3333,-.3333) node {\getNth{#2}{;}{7}};
    \draw (#1.center) ++(0,-.3333) node {\getNth{#2}{;}{8}};
    \draw (#1.center) ++(.3333,-.3333) node {\getNth{#2}{;}{9}};
}

\title{Strojové učení}
\author{Jindřich Matuška}
\institute[FI MU]{Faculty of Informatics, Masaryk University}
\date{5.\ prosince 2024}

\AtBeginSection[]
{
    \begin{frame}
        \frametitle{Obsah}
        \tableofcontents[currentsection]
    \end{frame}
}

\begin{document}

\frame{\titlepage}

\frame{
    \frametitle{Čas na odpovědníky}
}

\begin{frame}
    \frametitle{Obsah}
    \tableofcontents
\end{frame}

\frame{
    \frametitle{Strojové učení}

    Systémy, které se učí z poskytnutých dat

    \begin{itemize}
        \item Učení s učitelem --- pro vstupy jsou známé očekávané výstupy
        \item Učení bez učitele --- model se učí vzory v datech
        \item Zpětnovazebné učení --- akce provedené modelem jsou ohodnoceny,
            model se učí na základě těchto odměn
    \end{itemize}

    \vspace{1em}

    Základní úlohy strojového učení

    \begin{itemize}
        \item Klasifikace --- rozdělení nových dat do předem určených tříd
        \item Regrese --- modelování spojité proměnné
        \item Shlukování --- rozdělení dat do několika skupin se vzájemně podobnými atributy
    \end{itemize}
}

\begin{section}{Rozhodovací stromy, učení rozhodovacích stromů}

    \frame{
        \frametitle{Rozhodovací strom}

        Model především pro klasifikaci

        \begin{itemize}
            \item Vnitřní větve obsahují testovaný atribut
            \item Listové uzly obsahují hodnoty (třídy či diskrétní hodnoty)
            \item Hrany odpovídají možným hodnotám testovaného atributu
        \end{itemize}

        \vspace{1em}

        Rozhodovací strom je konzistentní s datasetem právě tehdy,
        když správně klasifikuje všechny příklady z datasetu.
    }

    \frame{
        \frametitle{Příklad 10.1.1}

        Mějme dataset $D$ zadaný následující tabulkou (řádky reprezentují jednotlivé
        příklady, sloupce atributy, závislá proměnná je \textit{Tenis})

		\begin{center}
		\begin{tabular}{ c c c | c } % chktex 44
		  \textit{Vlhkost} & \textit{Počasí} & \textit{Větrno} & \textit{Tenis} \\
			\midrule
		  vysoká & zataženo & ano & NE \\
		  střední  & slunečno & ne & ANO  \\
		  nízká  & slunečno & ano & ANO  \\
		  střední  & déšť & ne & NE  \\
		\end{tabular}
		\end{center}

        \begin{enumerate}[a)]
            \item Vytvořte (ručně) rozhodovací strom, který je konzistentní
                s tímto datasetem. Snažte se o co nejmenší strom.
            \item Jak by (konzistentní) strom vypadal, pokud by v $D$ byl navíc
                ještě příklad: \\
                \center{střední, slunečno, ne $\mid$ NE?}
        \end{enumerate}
    }

    \frame{
        \frametitle{Odbočka --- chyby, modely, validace}

        Data obsahují chyby měření.\\
        Hledáme funkci $T$, máme však jen nepřesná data $f(X)$
        \[f(X) \in T(X) + e(X)\]

        Vytvořit k datům konzistentní model je jednoduché,
        ale pravděpodobně overfittuje.
    }

    \begin{frame}[fragile]
        \frametitle{Učení rozhodovacích stromů}

        Problém nalezení nejmenšího konzistentního modelu je NP-úplný,
        budeme tedy používat heuristiky.

        \begin{lstlisting}[language=Python, basicstyle=\footnotesize]
def generateDT(data):
    node = Node()
    node.attribute = getBestAttribute(data)
    for value, subset in data.splitAttribute(node.attribute):
        node.childs[value] = generateDT(subset)
    return node
        \end{lstlisting}

    \end{frame}

    \frame{
        \frametitle{Entropie}

        Míra neuspořádanosti, nejistoty, informace.

        Nechť $D$ je dataset s příklady rozdělenými do $n$ tříd,
        $P_i$ je pravděpodobnost náhodného výběru prvku třídy $i$.
        \[E(D) = \sum_{i=1}^n - P_i \cdot \log_2(P_i)\]
    }

    \frame{
        \frametitle{Informační zisk}

        Jak moc se liší entropie původního datasetu a rozděleného datasetu.
        \vspace{1em}

        Nechť $D$ je dataset s atributem $A$ nabývajícím $k$ hodnot.
        Atribut $A$ rozděluje množinu $D$ na podmnožiny $D_1, \ldots, D_k$.
        \[Gain(D, A) = E(D) - \sum_{i=1}^k {||D_i|| \over ||D||}\cdot E(D_i)\]
    }

    \frame{
        \frametitle{Příklad 10.2.1, 10.2.2}

        Uvažujme dataset daný tabulkou. Nalezněte atribut, který by učící algoritmus
        zvolil jako rozhodovací atribut pro kořen stromu.
		\vspace{1em}

		Následně pro tento dataset vybudujte kompletní rozhodovací strom.

		\begin{center}
		\begin{tabular}{ c c c c c | c }
		  \textit{} & \textit{Vlhkost} & \textit{Počasí} & \textit{Větrno} & \textit{Teplota} & \textit{Tenis} \\
			\midrule
		  1 & vysoká & zataženo & ano & nižší & NE \\
		  2 & střední  & zataženo & ne & vyšší & ANO  \\
		  3 & nízká  & slunečno & ano & nižší & ANO  \\
		  4 & střední  & déšť & ne & nižší & NE  \\
		  5 & nízká  & slunečno & ano & vyšší & ANO  \\
		  6 & vysoká  & déšť & ne & vyšší & NE  \\
		  7 & střední  & slunečno & ano & nižší & ANO  \\
		\end{tabular}
		\end{center}
    }

\end{section}

\begin{section}{Perceptron, lineární klasifikace}

    \frame{
        \frametitle{Perceptron (neuron)}

        Lineární model, na výslednou hodnotu aplikuje \textit{aktivační funkci}.

        Pokud používáme perceptron jako klasifikátor, označujeme aktivační funkci jako prahovou funkci.
        \vspace{1em}

        Uvažme vstupní vektor $\vec x = \langle x_1, \ldots, x_n \rangle$
        a vektor vah $\vec w = \langle w_0, w_1, \ldots, w_n \rangle$.
        Výstup perceptronové jednotky s váhami $\vec w$ pro příklad $\vec x$ je
        \[C[\vec w](\vec x) = \begin{cases}
            1 & \text{pokud}\ w_0 + \sum_{i=1}^n w_i\cdot x_i = \vec w \cdot \tilde x \geq 0\\
            0 & \text{jinak}
        \end{cases}\]
        \vspace{1em}

        Perceptron často značíme pomocí následujícího schématu:
        % TODO: Schéma
    }

    \frame{
        \frametitle{Příklad 10.3.1}

        Pro každou z následujících logických operací definujte perceptron, který
        ji implementuje. Jako množinu vstupů uvažujte pouze vektory nad $\{0, 1\}$.

        \begin{itemize}
            \item Binární NAND
            \item Binární implikace
            \item $n$-ární disjunkce
        \end{itemize}
    }

    \frame{
        \frametitle{Geometrický význam perceptronu}


        \[C[\vec w](\vec x) = \begin{cases}
            1 & \text{pokud}\ w_0 + \sum_{i=1}^n w_i\cdot x_i = \vec w \cdot \tilde x \geq 0\\
            0 & \text{jinak}
        \end{cases}\]
        Perceptron je lineární separátor.\\
        Dělící nadrovina je množina bodů $\{ \vec x \mid \vec w \cdot \tilde x = 0\}$
        \vspace{1em}

        Perceptron je konzistentní s datasetem,
        pokud klasifikuje všechny prvky datasetu správně.
    }

    \frame{
        \frametitle{Online perceptronový algoritmus}

        Iterativní proces nastavení správných vah perceptronu.\\
        \vspace{1em}

        Pro tréninkový set $D = \{(\vec x_1, c_1), \ldots, (\vec x_p, c_p)\}$,
        kde $c_i$ je opravdová hodnota příkladu $x_i$ je sekvence vektorů vah
        počítána následovně:
        \begin{itemize}
            \item $\vec w^{(0)}$ je inicializována náhodně s hodnotami okolo 0,
            \item $\vec w^{(t+1)} = \vec w^{(t)} - \alpha\cdot (C[\vec w^{(t)}](\vec x_k) - c_k) \cdot \tilde x_k$,
        \end{itemize}
        kde $k = (t \mod p) + 1$,\\ $0 < \alpha \leq 1$ je konstanta učení
    }

    \frame{
        \frametitle{Příklad 10.4.2}

        Mějme trénovací sadu
        \[D=\{(\langle 3,-1 \rangle, 1), (\langle 2,1 \rangle,1), (\langle 0,3 \rangle,0) \}\]
        Aplikujte perceptronový algoritmus, dokud nenalezne separující nadrovinu.
        Uvažujte $\vec{w}^{(0)} = \langle 0, -2, 1 \rangle$ a $\alpha=1$.
        Na závěr načrtněte dělící přímku.
    }

\end{section}

\begin{section}{Základy lineární regrese}

    \frame{
        \frametitle{Lineární regrese}

        Klasifikace --- rozřazení dat do tříd\\
        Regrese --- modelování neznámé funkce
        \vspace{1em}

        Lineární regrese používá model podobný perceptronu:
        \[\vec R[\vec w](\vec x) = \vec w \cdot \tilde x\]

        Data prokládáme co nejlepším lineárním modelem
    }

    \frame{
        \frametitle{Chybová funkce}

        Chybová funkce vyjadřuje jak moc se model blíží datům.\\
        Menší chyba znamená lepší model.
        \vspace{1em}

        \textbf{Kvadratická chyba}\\
        Uvažme dataset $D = \{(\vec x_1, f_1), \ldots, (\vec x_p, f_p)\}$,
        kde $f_i$ je opravdová hodnota příkladu $\vec x_i$.
        Kvadratická chyba pro model s váhami $\vec w$ je
        \[E(\vec w) = {1\over 2} \cdot \sum_{k=1}^p {(R[\vec w](\vec x_k) - f_k)}^2
        = {1\over 2} \cdot \sum_{k=1}^p {(\vec w \cdot \tilde x_k - f_k)}^2\]
    }

\end{section}
\end{document}  % chktex 17
