{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8Cj_X1VnmC0"
   },
   "source": [
    "# PB016: Artificial intelligence I, labs 12 - Deep learning\n",
    "\n",
    "Today's topic is a quick and dirty introduction into deep learning. We'll focus namely on:\n",
    "1. __Dummy deep learning pipeline__\n",
    "2. __Developing your own deep learning classifier__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjLOiihWnmC1"
   },
   "source": [
    "---\n",
    "\n",
    "## 1. Dummy deep learning pipeline\n",
    "\n",
    "__Basic facts__\n",
    "- Deep learning consists of designing, training and validating machine learning models based on various [neural architectures](https://en.wikipedia.org/wiki/Types_of_artificial_neural_networks) that typically involve multiple (hidden) layers consisting of many neural computing units (a simple example of one unit is [perceptron](https://en.wikipedia.org/wiki/Perceptron), such as the one we implemented in the previous labs).\n",
    "- An example of a deep learning architecture:\n",
    "\n",
    "<img src=\"https://www.fi.muni.cz/~novacek/courses/pb016/labs/img/stacked-representation.png\" alt=\"architecture\" width=\"550px\" title=\"Original image source: Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. ”Deep learning.” MIT press, 2016. (Chap. 1) License: Probably OK to use for academic purposes; for any other use, contact the publisher (MIT Press).\"/>\n",
    "\n",
    "- A number of libraries seamlessly integrating with parallel computational architectures is available for developing deep learning models. Some of the popular examples are:\n",
    " - [PyTorch](https://pytorch.org/) - originally a general-purpose ML library written in C, now a state-of-the-art deep learning framework with relatively easy-to-use Python (and C++) abstraction layers.\n",
    " - [TensorFlow](https://www.tensorflow.org/) - a general-purpose, highly optimised library for multilinear algebra and statistical learning.\n",
    " - [Keras](https://keras.io/) - formerly a separate project, now an abstraction layer for user-friendly development of deep learning models integrated with TensorFlow, PyTorch and [JAX](https://jax.readthedocs.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtRA1s8qdz2e"
   },
   "source": [
    "### A warm-up task - predicting onset of diabetes using Keras\n",
    "- This is based on a widely used PIMA Indians dataset - a classic machine learning sandbox data described in detail for instance [here](https://towardsdatascience.com/pima-indian-diabetes-prediction-7573698bd5fe).\n",
    "- The task is to use that dataset to train a classifier for predicting whether or not a person develops diabetes.\n",
    "- This is based on a number of characteristics (i.e., features) like blood pressure or body mass index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtWS3vDyd9pI"
   },
   "source": [
    "#### Loading the data using [pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/tutorials.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DvmXGN9OWh3O"
   },
   "outputs": [],
   "source": [
    "# importing the library for handy CSV file processing\n",
    "import pandas as pd\n",
    "\n",
    "# loading the data, in CSV format, from the web\n",
    "dataframe = pd.read_csv('https://www.fi.muni.cz/~novacek/courses/pb016/labs/data/12/example/diabetes.csv')\n",
    "\n",
    "# checking the first few rows of the CSV\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzQWHmkmg3BZ"
   },
   "source": [
    "### Creating the data structures representing features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dYwVJCQng8gJ"
   },
   "outputs": [],
   "source": [
    "# getting just the Outcome column as the vector of labels\n",
    "# - note that the column contains 0, 1 values that correspond to negaive\n",
    "#   (no diabetes developed) and positive (diabetes developed) example labels,\n",
    "#   respectively\n",
    "df_labels = dataframe.Outcome.values.astype(float)\n",
    "# the features are the data minus the label vector\n",
    "# - this contains the remaining features present in the data\n",
    "df_features = dataframe.drop('Outcome',axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diBlgxl_hTCS"
   },
   "source": [
    "### Splitting the data into train and test sets using [scikit-learn](https://sklearn.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kL8i6lBXhWfi"
   },
   "outputs": [],
   "source": [
    "# importing a convenience data splitting function from scikit-learn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# computing a random 80-20 split (80% training data, 20% of remaining\n",
    "# \"unseen\" data for testing the model trained on the 80%)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_features,df_labels,\\\n",
    "                                                    test_size=0.2,\\\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-r1dRs5fnRC"
   },
   "source": [
    "### Training a baseline, classical machine learning model (logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JGuqUIN3fmQL"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "logreg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cmZkFuIf01c"
   },
   "source": [
    "### Evaluating the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MxT6Axjdf2qp"
   },
   "outputs": [],
   "source": [
    "# using the trained model to predict the test labels\n",
    "y_pred = logreg.predict(x_test)\n",
    "\n",
    "# importing stuff needed to display a confusion matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "# making plots prettier\n",
    "import seaborn as sns\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    logreg, x_test, y_test, xticks_rotation=\"vertical\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0UFQvfFJf-3U"
   },
   "outputs": [],
   "source": [
    "# importing some widely-used scoring functions from scikit-klearn\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# computing the precision, recall and F1 scores from the predictions\n",
    "score_p = precision_score(y_test, y_pred, average='macro')\n",
    "score_r = recall_score(y_test, y_pred, average='macro')\n",
    "score_f = f1_score(y_test, y_pred, average='macro')\n",
    "# using a scoring method of the model itself (to compute accuracy)\n",
    "score_a = logreg.score(x_test, y_test)\n",
    "\n",
    "# printing out the scores\n",
    "print('Various scores of the logistic regression classifier on the test set')\n",
    "# the number of correct predictions (true positives and true negatives)\n",
    "# divided by the number of all predictions\n",
    "print('  - accuracy :', score_a)\n",
    "# the number of patients correctly classified as high risk\n",
    "# divided by the number of all patients classified as high risk\n",
    "print('  - precision:', score_p)\n",
    "# the number of patients correctly classified as high risk,\n",
    "# divided by the number of all patients that really are high risk\n",
    "print('  - recall   :', score_r)\n",
    "# aggregation of the precision and recall values (harmonic mean)\n",
    "print('  - F1       :', score_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paAJQMT6iAX2"
   },
   "source": [
    "### Creating a [Keras](https://keras.io/) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x3oWiAHwiC2y"
   },
   "outputs": [],
   "source": [
    "# adapted from:\n",
    "#   - https://www.kaggle.com/code/atulnet/pima-diabetes-keras-implementation\n",
    "\n",
    "# importing the basics from Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "\n",
    "# a model for simple sequential stacking of layers\n",
    "model = Sequential()\n",
    "\n",
    "# 1st layer: implicit input layer corresponding to the feature vector of size 8\n",
    "model.add(Input(shape=(8,)))\n",
    "# 2nd layer: 100 fully connected nodes, a simple non-linear activation\n",
    "#            (ReLU - rectified linear unit)\n",
    "model.add(Dense(100, activation='relu'))\n",
    "# output layer: dim=1, sigmoid activation\n",
    "#               (probability of the input characteristic of the positive class)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compiling the model with the binary cross-entropy loss (predicting 0/1)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXoiiGpCjaD0"
   },
   "source": [
    "### Training the created model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R0k0oJzyjb22"
   },
   "outputs": [],
   "source": [
    "# simply calling the fit function, training on the training data and validating\n",
    "# on the test data after each epoch\n",
    "model.fit(x_train,y_train,epochs=30,\\\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0pTXap1nBq5"
   },
   "source": [
    "- Interpreting the results\n",
    " - Not too great:\n",
    "   - The loss is barely being optimised after about 10 epochs\n",
    "   - The validation accuracy is worse then the classical ML baseline (barely over 0.7 in most runs as opposed to nearly 0.76)\n",
    " - The reasons:\n",
    "   - More or less default settings of the model\n",
    "   - More importantly, though, there's no preprocessing of the rather noisy and skewed input data (see for instance [this](https://towardsdatascience.com/pima-indian-diabetes-prediction-7573698bd5fe) or [this](https://www.kaggle.com/code/atulnet/pima-diabetes-keras-implementation) blog post, where examples of detailed exploratory analysis and input data transformations are described)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lr1fmV_UnmDZ"
   },
   "source": [
    "---\n",
    "\n",
    "## 2. Developing your own deep learning classifier\n",
    "- Your task is to predict which passengers survived the Titanic disaster, as described in the [Kaggle](https://www.kaggle.com) challenge on [Machine Learning from Disaster](https://www.kaggle.com/c/titanic/overview).\n",
    "\n",
    "![titanic](https://www.fi.muni.cz/~novacek/courses/pb016/labs/img/titanic.jpg)\n",
    "\n",
    "- Split into groups (min 2, max 4 people).\n",
    "- Register at [Kaggle](https://www.kaggle.com/) so that you can officially participate in the [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/overview) competition  (one account per group is enough).\n",
    "- Then, use a deep learning library ([Keras](https://keras.io/) might be the easiest option for newbies, but feel free to use for instance [PyTorch](https://pytorch.org/) if that's what you're already comfortable with) to solve the Titanic survivors' prediction problem as follows:\n",
    " - Get the challenge [data](https://www.kaggle.com/c/titanic/data) via the URLs in the notebook below.\n",
    " - Design a simple neural model for classification of (non)survivors using Keras.\n",
    " - Train the model on the `train.csv` dataset (after possibly preprocessing the data).\n",
    " - Use the trained model to predict the labels of the set `test.csv` (i.e., the values ​​of the column _\"survived\"_; for more details, see the competition documentation itself).\n",
    " - [Upload results](https://www.kaggle.com/c/titanic/submit) on Kaggle.\n",
    "- Discuss your model and score with the lab tutor! The collaborating members of the group with the best relative results and/or an interesting/elegant/efficient/unusual model can earn bonus points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWIcu-WFrB-N"
   },
   "source": [
    "### Loading the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzC3yqpHniPM"
   },
   "outputs": [],
   "source": [
    "# importing pandas, just in case it wasn't imported before\n",
    "import pandas as pd\n",
    "\n",
    "# loading the train and test data using pandas\n",
    "\n",
    "df_train = pd.read_csv('https://www.fi.muni.cz/~novacek/courses/pb016/labs/data/12/titanic/train.csv',\\\n",
    "                       index_col='PassengerId')\n",
    "df_test = pd.read_csv('https://www.fi.muni.cz/~novacek/courses/pb016/labs/data/12/titanic/test.csv', \\\n",
    "                      index_col='PassengerId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IA4hfwGR26oP"
   },
   "source": [
    "### Checking out the train and test data contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "agY80lGO2_Sa"
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3RAwtMw53LMa"
   },
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7jqJu5SrGec"
   },
   "source": [
    "### Developing the model itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jESxfpu-rIzx"
   },
   "outputs": [],
   "source": [
    "# TODO - YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diP58FYa-TYD"
   },
   "source": [
    "### Notes on the solution\n",
    "- Feel free to get inspired on the web, but make sure you understand what you're doing when using someone else's (be it human or AI) code.\n",
    "- A practical note on getting the submission file to be uploaded to Kaggle, if you're working in Google Colab:\n",
    " - You can create the CSV in your virtual environment, for instance using the `submission.to_csv('submission.csv', index=False)` command, assuming the `submission` variable is a _pandas_ data frame object.\n",
    " - Then you can simply download it by first importing the `files` module by the `from google.colab import files` line, and then using the module with the `files.download('submission.csv')` line to store the data on your local machine.\n",
    " - To save you some effort, the following code cell contains the Google Colab code that can take care of the above steps, assuming you have generated the `predictions` variable from the Titanic test set using your trained model's `predict()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UM4oXPXI8O0l"
   },
   "outputs": [],
   "source": [
    "# the pandas data frame with the results\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': df_test.index,\n",
    "    'Survived': predictions,\n",
    "})\n",
    "\n",
    "# storing the submissions as CSV\n",
    "submission.sort_values('PassengerId', inplace=True)\n",
    "submission.to_csv('submission-example.csv', index=False)\n",
    "\n",
    "# downloading the created CSV file locally\n",
    "from google.colab import files\n",
    "files.download('submission-example.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "yLjsBHRLnmDa",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "---\n",
    "\n",
    "#### _Final note_ - the materials used in this notebook are original works credited and licensed as follows:\n",
    "- Image of Titanic:\n",
    " - Retrieved from [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:St%C3%B6wer_Titanic.jpg)\n",
    " - Author: Willy Stöwer (image reproduction)\n",
    " - License: none (or [Public Domain](https://en.wikipedia.org/wiki/public_domain))\n",
    "- Image of the DL architecture:\n",
    " - See the inline note associated with the image itself."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
